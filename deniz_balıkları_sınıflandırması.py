# -*- coding: utf-8 -*-
"""Deniz BalÄ±klarÄ± SÄ±nÄ±flandÄ±rmasÄ±

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j6yhlOJhc9TdxTPNXZ-9CGZFoZ3ii2O2
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
crowww_a_large_scale_fish_dataset_path = kagglehub.dataset_download('crowww/a-large-scale-fish-dataset')

print('Data source import complete.')

from google.colab import drive
drive.mount('/content/drive')

"""ğŸŸ**Derin Ã–ÄŸrenmeye GiriÅŸ: Fish Dataset ile AdÄ±m AdÄ±m Ã–ÄŸrenme**
Bu notebook, derin Ã¶ÄŸrenmeye sÄ±fÄ±rdan baÅŸlayanlar ve bilgilerini geliÅŸtirmek isteyenler iÃ§in hazÄ±rlanmÄ±ÅŸ kapsamlÄ± bir rehberdir. Her bÃ¶lÃ¼m (PART) ayrÄ± bir konuya odaklanÄ±r ve hem teorik aÃ§Ä±klama hem de uygulamalÄ± kod iÃ§erir.

ğŸ¯ *Ã–ÄŸrenme Hedefleri*

Derin Ã¶ÄŸrenmenin temel kavramlarÄ±nÄ± anlamak
TensorFlow/Keras ile model oluÅŸturmayÄ± Ã¶ÄŸrenmek
FarklÄ± katman tÃ¼rlerini ve aktivasyon fonksiyonlarÄ±nÄ± keÅŸfetmek
CNN mimarisini anlamak ve uygulamak
Transfer Learning tekniklerini Ã¶ÄŸrenmek
Model performansÄ±nÄ± optimize etmeyi Ã¶ÄŸrenmek

ğŸ“Œ *Ä°Ã§indekiler*


* PART 1: Derin Ã–ÄŸrenmeye GiriÅŸ & Dataset HazÄ±rlÄ±ÄŸÄ±
* PART 2: Dense, Flatten KatmanlarÄ± ve Aktivasyon FonksiyonlarÄ±na GiriÅŸ
* PART 3: Aktivasyon FonksiyonlarÄ±nÄ±n KarÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±
* PART 4: Loss FonksiyonlarÄ± ve Optimizasyon
* PART 5: Regularization Teknikleri
* PART 6: CNN KatmanlarÄ± (Conv2D ve Pooling)
* PART 7: Transfer Learning
* PART 8: Hyperparameter Tuning

# ğŸ“– PART 1: Derin Ã–ÄŸrenmeye GiriÅŸ & Dataset HazÄ±rlÄ±ÄŸÄ±
ğŸ¤– **Derin Ã–ÄŸrenme Nedir?**
Derin Ã–ÄŸrenme (Deep Learning), yapay zeka alanÄ±nÄ±n en gÃ¼Ã§lÃ¼ dallarÄ±ndan biridir. Ä°nsan beynindeki nÃ¶ronlarÄ±n Ã§alÄ±ÅŸma prensibinden ilham alarak:

Katmanlar halinde bilgi iÅŸleme
Otomatik Ã¶zellik Ã§Ä±karma (Feature Extraction)
BÃ¼yÃ¼k veri setlerinden kalÄ±plarÄ± Ã¶ÄŸrenme

ğŸ¯**Temel Kavramlar:**

NÃ¶ron: Temel iÅŸlem birimi
Katman (Layer): NÃ¶ronlarÄ±n grubu
AÄŸÄ±rlÄ±k (Weight): BaÄŸlantÄ±larÄ±n gÃ¼cÃ¼
Bias: Karar eÅŸiÄŸi
Aktivasyon: NÃ¶ronun Ã§Ä±kÄ±ÅŸÄ±nÄ± belirleme

ğŸ **Fish Dataset HakkÄ±nda**
Bu projede balÄ±k tÃ¼rlerini sÄ±nÄ±flandÄ±rmak iÃ§in bÃ¼yÃ¼k Ã¶lÃ§ekli bir balÄ±k veri seti kullanÄ±yoruz. Dataset'in Ã¶zellikleri:

Ã‡oklu balÄ±k tÃ¼rleri (Ã§eÅŸitli deniz canlÄ±larÄ±)
YÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ gÃ¶rÃ¼ntÃ¼ler
Ground truth (doÄŸru etiketler) dahil
"""

import os
# CUDA ayarlarÄ±nÄ± TensorFlow import edilmeden Ã¶nce yapÄ±n
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'   # Gereksiz loglarÄ± gizle
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
os.environ['CUDA_CACHE_DISABLE'] = '0'

# Gerekli kÃ¼tÃ¼phaneleri import et
import tensorflow as tf
#tf.config.set_visible_devices([], 'GPU')  # GPU'larÄ± gizle (CPU'da Ã§alÄ±ÅŸÄ±r)

import matplotlib.pyplot as plt
import numpy as np
import shutil
import tempfile
from tensorflow.keras import layers, models

# Matplotlib ayarlarÄ±
plt.style.use('default')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

print("ğŸš€ Gerekli kÃ¼tÃ¼phaneler yÃ¼klendi!")
print(f"TensorFlow versiyon: {tf.__version__}")

# Dataset yolu - Kaggle'daki Fish Dataset
data_dir = "../input/a-large-scale-fish-dataset/Fish_Dataset/Fish_Dataset"

def analyze_dataset_structure(data_path):
    """
    Dataset yapÄ±sÄ±nÄ± analiz eder ve rapor oluÅŸturur
    """
    print("=" * 60)
    print("ğŸ“Š VERÄ° SETÄ° YAPISI ANALÄ°ZÄ°")
    print("=" * 60)

    try:
        # Ana dizindeki klasÃ¶rleri listele
        main_contents = os.listdir(data_path)
        species_count = len(main_contents)

        print(f"ğŸ£ Toplam balÄ±k tÃ¼rÃ¼: {species_count}")
        print(f"ğŸ“ Ana dizin: {data_path}")
        print()

        total_images = 0
        species_info = {}

        for species in main_contents:
            species_path = os.path.join(data_path, species)

            if os.path.isdir(species_path):
                subfolders = os.listdir(species_path)
                print(f"ğŸ  {species}:")

                for subfolder in subfolders:
                    subfolder_path = os.path.join(species_path, subfolder)
                    if os.path.isdir(subfolder_path):
                        # GÃ¶rsel dosyalarÄ±nÄ± say
                        image_files = [f for f in os.listdir(subfolder_path)
                                     if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
                        image_count = len(image_files)

                        if not subfolder.endswith(' GT'):  # Ground Truth klasÃ¶rÃ¼ deÄŸilse
                            total_images += image_count
                            species_info[species] = image_count

                        folder_type = "ğŸ–¼ï¸ GÃ¶rseller" if not subfolder.endswith(' GT') else "ğŸ¯ Ground Truth"
                        print(f"    â”œâ”€â”€ {folder_type}: {image_count} dosya")

                print()

        print("=" * 60)
        print("ğŸ“ˆ Ã–ZET BÄ°LGÄ°LER")
        print("=" * 60)
        print(f"ğŸ¯ Toplam gÃ¶rsel sayÄ±sÄ±: {total_images}")
        print(f"ğŸ£ Ortalama tÃ¼r baÅŸÄ±na gÃ¶rsel: {total_images // species_count}")
        print(f"ğŸ“Š En Ã§ok gÃ¶rseli olan tÃ¼r: {max(species_info.items(), key=lambda x: x[1])}")
        print(f"ğŸ“‰ En az gÃ¶rseli olan tÃ¼r: {min(species_info.items(), key=lambda x: x[1])}")

        return species_info, total_images

    except Exception as e:
        print(f"âŒ Hata: {e}")
        return {}, 0

# Dataset analizini Ã§alÄ±ÅŸtÄ±r
species_info, total_images = analyze_dataset_structure(data_dir)

import os
import shutil
import tempfile

# Correcting the data_dir path
# The Kaggle dataset path is provided by kagglehub in 'crowww_a_large_scale_fish_dataset_path'
# Assuming the structure is: <kagglehub_path>/Fish_Dataset/Fish_Dataset/...
data_dir = os.path.join(crowww_a_large_scale_fish_dataset_path, "Fish_Dataset", "Fish_Dataset")

def prepare_dataset_structure(source_dir):
    """
    Ä°Ã§ iÃ§e klasÃ¶r yapÄ±sÄ±nÄ± TensorFlow iÃ§in uygun hale getirir

    Orijinal yapÄ±: Fish_Dataset/BalikTuru/BalikTuru/resim.jpg
    Hedef yapÄ±: temp_dir/BalikTuru/resim.jpg
    """
    print("ğŸ”„ Dataset yapÄ±sÄ± dÃ¼zenleniyor...")

    # GeÃ§ici dizin oluÅŸtur
    temp_dir = tempfile.mkdtemp()
    print(f"ğŸ“ GeÃ§ici dizin: {temp_dir}")

    species_dirs = [d for d in os.listdir(source_dir)
                   if os.path.isdir(os.path.join(source_dir, d))]

    total_copied = 0

    for species in species_dirs:
        species_path = os.path.join(source_dir, species)
        subfolders = os.listdir(species_path)

        # GT olmayan (gÃ¶rsel) klasÃ¶rÃ¼ bul
        image_folder = None
        for subfolder in subfolders:
            if not subfolder.endswith(' GT'):
                image_folder = subfolder
                break

        if image_folder:
            source_images = os.path.join(species_path, image_folder)
            target_species = os.path.join(temp_dir, species)

            if os.path.exists(source_images):
                # TÃ¼r klasÃ¶rÃ¼nÃ¼ oluÅŸtur
                os.makedirs(target_species, exist_ok=True)

                # GÃ¶rselleri kopyala
                copied_count = 0
                for img_file in os.listdir(source_images):
                    if img_file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
                        src = os.path.join(source_images, img_file)
                        dst = os.path.join(target_species, img_file)
                        try:
                            shutil.copy2(src, dst)
                            copied_count += 1
                        except Exception as e:
                            print(f"    âš ï¸ {img_file} kopyalanamadÄ±: {e}")

                total_copied += copied_count
                print(f"    âœ… {species}: {copied_count} gÃ¶rsel kopyalandÄ±")

    print(f"ğŸ‰ Toplam {total_copied} gÃ¶rsel baÅŸarÄ±yla dÃ¼zenlendi!")
    return temp_dir

# Dataset yapÄ±sÄ±nÄ± dÃ¼zenle
print("ğŸ“ Dataset hazÄ±rlÄ±ÄŸÄ± baÅŸlÄ±yor...")
processed_data_dir = prepare_dataset_structure(data_dir)

# Model parametreleri
IMG_SIZE = (128, 128)  # GÃ¶rsel boyutu (daha hÄ±zlÄ± eÄŸitim iÃ§in kÃ¼Ã§Ã¼k)
BATCH_SIZE = 32        # Her seferde iÅŸlenecek gÃ¶rsel sayÄ±sÄ±
VALIDATION_SPLIT = 0.2 # %20 doÄŸrulama verisi
RANDOM_SEED = 123      # Tekrarlanabilir sonuÃ§lar iÃ§in

print("ğŸ›ï¸ MODEL PARAMETRELERÄ°")
print("=" * 40)
print(f"ğŸ“ GÃ¶rsel boyutu: {IMG_SIZE}")
print(f"ğŸ“¦ Batch boyutu: {BATCH_SIZE}")
print(f"ğŸ² Validation split: {VALIDATION_SPLIT}")
print(f"ğŸŒ± Random seed: {RANDOM_SEED}")

# EÄŸitim veri setini oluÅŸtur
print("\nğŸ”„ Veri setleri yÃ¼kleniyor...")
tf.keras.layers.Rescaling(1./255) #Modelin daha hÄ±zlÄ± ve doÄŸru Ã§alÄ±ÅŸmasÄ± iÃ§in renkleri basit sayÄ±lara dÃ¶nÃ¼ÅŸtÃ¼rme

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    processed_data_dir,
    validation_split=VALIDATION_SPLIT,
    subset="training",
    seed=RANDOM_SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='int'  # SÄ±nÄ±f etiketleri iÃ§in integer kullan
)

# DoÄŸrulama veri setini oluÅŸtur
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    processed_data_dir,
    validation_split=VALIDATION_SPLIT,
    subset="validation",
    seed=RANDOM_SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='int'
)

# SÄ±nÄ±f bilgileri
class_names = train_ds.class_names
num_classes = len(class_names)

print(f"âœ… Veri setleri hazÄ±r!")
print(f"ğŸ·ï¸ Bulunan sÄ±nÄ±flar ({num_classes} adet):")
for i, class_name in enumerate(class_names):
    print(f"    {i}: {class_name}")

# Veri setinden Ã¶rnek gÃ¶rseller gÃ¶ster
def display_sample_images(dataset, class_names, num_samples=9):
    """
    Veri setinden Ã¶rnek gÃ¶rselleri gÃ¶sterir
    """
    plt.figure(figsize=(15, 15))

    # Ä°lk batch'i al
    for images, labels in dataset.take(1):
        # GÃ¶sterilecek Ã¶rnek sayÄ±sÄ±nÄ± belirle
        samples_to_show = min(num_samples, len(images))

        # Grid boyutunu hesapla
        grid_size = int(np.ceil(np.sqrt(samples_to_show)))

        for i in range(samples_to_show):
            ax = plt.subplot(grid_size, grid_size, i + 1)

            # GÃ¶rseli gÃ¶ster
            image = images[i].numpy().astype("uint8")
            plt.imshow(image)

            # SÄ±nÄ±f etiketi ekle
            label_index = labels[i].numpy()
            if label_index < len(class_names):
                title = f"{class_names[label_index]}"
                plt.title(title, fontsize=12, pad=10)
            else:
                plt.title(f"Label: {label_index}", fontsize=12, pad=10)

            plt.axis("off")

    plt.tight_layout()
    plt.suptitle("ğŸ–¼ï¸ Ã–rnek GÃ¶rÃ¼ntÃ¼ler", fontsize=16, y=1.02)
    plt.show()

# Ã–rnek gÃ¶rselleri gÃ¶ster
print("ğŸ–¼ï¸ Ã–rnek gÃ¶rÃ¼ntÃ¼ler yÃ¼kleniyor...")
display_sample_images(train_ds, class_names, num_samples=12)

# Veri seti performansÄ±nÄ± optimize et
print("âš¡ Veri seti performansÄ± optimize ediliyor...")

AUTOTUNE = tf.data.AUTOTUNE

# Cache ve prefetch ile performansÄ± artÄ±r
train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

print("âœ… Veri setleri optimize edildi!")
print(f"ğŸš€ EÄŸitim seti batch sayÄ±sÄ±: {len(train_ds)}")
print(f"ğŸ¯ DoÄŸrulama seti batch sayÄ±sÄ±: {len(val_ds)}")
print(f"ğŸ“Š Tahmini eÄŸitim gÃ¶rsel sayÄ±sÄ±: {len(train_ds) * BATCH_SIZE}")
print(f"ğŸ“Š Tahmini doÄŸrulama gÃ¶rsel sayÄ±sÄ±: {len(val_ds) * BATCH_SIZE}")

"""# ğŸ“– PART 2: Dense, Flatten KatmanlarÄ± ve Aktivasyon FonksiyonlarÄ±na GiriÅŸ
ğŸ§  **Temel Katman TÃ¼rleri**
1. *Dense Layer (Tam BaÄŸlantÄ±lÄ± Katman)*

Her nÃ¶ronun Ã¶nceki katmandaki tÃ¼m nÃ¶ronlarla baÄŸlantÄ±lÄ± olduÄŸu katman

* Parametre sayÄ±sÄ±: **(input_size Ã— output_size) + bias_terms**

* KullanÄ±m alanÄ±: SÄ±nÄ±flandÄ±rma ve regresyon problemlerinin son katmanlarÄ±

2. *Flatten Layer*
   
BilgisayarÄ±n gÃ¶rsel matrisini (en, boy, renk kanalÄ±) alÄ±p, Dense katmanÄ±nÄ±n anlayacaÄŸÄ± ÅŸekilde tek, uzun bir Ã§izgi (vektÃ¶r) haline getirir.

Ã‡ok boyutlu veriyi (Ã¶rn: 28Ã—28Ã—3) tek boyutlu vektÃ¶re (2352,) Ã§evirir

* Parametre yok - sadece veri formatÄ±nÄ± deÄŸiÅŸtirir

* CNN'den Dense'e geÃ§iÅŸte zorunlu

3. *Aktivasyon FonksiyonlarÄ±*
Neural network'lerin doÄŸrusal olmayan kararlar almasÄ±nÄ± saÄŸlar:


* ReLU: max(0, x) - En popÃ¼ler, hÄ±zlÄ± hesaplama

* Sigmoid: 1/(1+e^(-x)) - 0-1 arasÄ± Ã§Ä±kÄ±ÅŸ, binary sÄ±nÄ±flandÄ±rma

* Softmax: Ã‡oklu sÄ±nÄ±f olasÄ±lÄ±klarÄ±, toplamlarÄ± 1 (tahmin edilen tÃ¼m balÄ±k tÃ¼rlerinin olasÄ±lÄ±klarÄ±nÄ± toplayÄ±p 1 yapacak ÅŸekilde daÄŸÄ±tmasÄ±)

* Tanh: -1 ile 1 arasÄ± Ã§Ä±kÄ±ÅŸ, sÄ±fÄ±r merkezli
"""

print("ğŸ—ï¸ Ä°LK NEURAL NETWORK MODELÄ° OLUÅTURULUYOR")
print("=" * 60)

def create_simple_model(input_shape, num_classes, hidden_units=128):
    """
    Basit Dense katmanlÄ± model oluÅŸturur

    Mimari: Input -> Flatten -> Dense(ReLU) -> Dense(Softmax)
    """
    model = models.Sequential([
        # GiriÅŸ katmanÄ± - gÃ¶rsel boyutunu belirt
        layers.Input(shape=input_shape),

        # Veri normalleÅŸtirme - piksel deÄŸerlerini 0-1 arasÄ±na Ã§ek
        layers.Rescaling(1./255, name="normalization"),

        # Ã‡ok boyutlu gÃ¶rÃ¼ntÃ¼yÃ¼ 1D vektÃ¶re Ã§evir
        layers.Flatten(name="flatten"),

        # Gizli katman - Ã¶zellik Ã¶ÄŸrenme
        layers.Dense(hidden_units, activation='relu', name="hidden_layer"),

        # Ã‡Ä±kÄ±ÅŸ katmanÄ± - sÄ±nÄ±f olasÄ±lÄ±klarÄ±
        layers.Dense(num_classes, activation='softmax', name="output_layer")
    ], name="SimpleNeuralNetwork")

    return model

# Model oluÅŸtur
simple_model = create_simple_model(
    input_shape=IMG_SIZE + (3,),  # RGB gÃ¶rseller iÃ§in (128, 128, 3)
    num_classes=num_classes,
    hidden_units=128
)

# Model Ã¶zetini gÃ¶ster
print("ğŸ“‹ MODEL MÄ°MARÄ°SÄ°:")
simple_model.summary()

# Model yapÄ±sÄ±nÄ± gÃ¶rselleÅŸtir (opsiyonel)
print(f"\nğŸ” MODEL DETAYLARI:")
print(f"   ğŸ“ GiriÅŸ boyutu: {IMG_SIZE + (3,)}")
print(f"   ğŸ¯ SÄ±nÄ±f sayÄ±sÄ±: {num_classes}")
print(f"   ğŸ§  Gizli katman nÃ¶ron sayÄ±sÄ±: 128")
print(f"   ğŸ“Š Toplam parametre sayÄ±sÄ±: {simple_model.count_params():,}")

# Modeli derleme (Compile)
print("âš™ï¸ MODEL DERLENÄ°YOR...")

# Optimizasyon ayarlarÄ±
LEARNING_RATE = 0.001
OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)
LOSS_FUNCTION = 'sparse_categorical_crossentropy'  # Integer etiketler iÃ§in
METRICS = ['accuracy']

simple_model.compile(
    optimizer=OPTIMIZER,
    loss=LOSS_FUNCTION,
    metrics=METRICS
)

print("âœ… Model baÅŸarÄ±yla derlendi!")
print(f"ğŸ¯ Optimizer: Adam (lr={LEARNING_RATE})")
print(f"ğŸ“‰ Loss fonksiyonu: {LOSS_FUNCTION}")
print(f"ğŸ“Š Metrikler: {METRICS}")

# Model eÄŸitimi
print("ğŸš€ MODEL EÄÄ°TÄ°MÄ° BAÅLIYOR...")
print("=" * 50)

# EÄŸitim parametreleri
EPOCHS = 5  # HÄ±zlÄ± test iÃ§in az epoch
VERBOSE = 1  # EÄŸitim durumunu gÃ¶ster

# Early stopping callback (opsiyonel - overfitting'i Ã¶nler)
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True,
    verbose=1
)

# Model checkpoint (en iyi modeli kaydet)
checkpoint = tf.keras.callbacks.ModelCheckpoint(
    'best_simple_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1
)

print(f"ğŸ“š Epoch sayÄ±sÄ±: {EPOCHS}")
print(f"â° Tahmini sÃ¼re: ~{EPOCHS * 2} dakika")
print()

# EÄŸitimi baÅŸlat
import time
start_time = time.time()

history = simple_model.fit(
    train_ds,
    epochs=EPOCHS,
    validation_data=val_ds,
    callbacks=[early_stopping],  # checkpoint kaldÄ±rÄ±ldÄ± (dosya sistemi kÄ±sÄ±tlamasÄ±)
    verbose=VERBOSE
)

end_time = time.time()
training_time = end_time - start_time

print(f"\nâœ… EÄÄ°TÄ°M TAMAMLANDI!")
print(f"â±ï¸ Toplam sÃ¼re: {training_time:.2f} saniye")
print(f"â±ï¸ Epoch baÅŸÄ±na ortalama: {training_time/EPOCHS:.2f} saniye")

# EÄŸitim sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtir
def plot_training_history(history, title="Model EÄŸitim SonuÃ§larÄ±"):
    """
    EÄŸitim geÃ§miÅŸini gÃ¶rselleÅŸtirir
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    epochs_range = range(1, len(history.history['accuracy']) + 1)

    # Accuracy grafiÄŸi
    ax1.plot(epochs_range, history.history['accuracy'], 'b-',
             label='EÄŸitim DoÄŸruluÄŸu', linewidth=2, marker='o')
    ax1.plot(epochs_range, history.history['val_accuracy'], 'r-',
             label='DoÄŸrulama DoÄŸruluÄŸu', linewidth=2, marker='s')

    ax1.set_title('Model DoÄŸruluÄŸu', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('DoÄŸruluk')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1)

    # Loss grafiÄŸi
    ax2.plot(epochs_range, history.history['loss'], 'b-',
             label='EÄŸitim KaybÄ±', linewidth=2, marker='o')
    ax2.plot(epochs_range, history.history['val_loss'], 'r-',
             label='DoÄŸrulama KaybÄ±', linewidth=2, marker='s')

    ax2.set_title('Model KaybÄ±', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('KayÄ±p')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.suptitle(title, fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()

# EÄŸitim geÃ§miÅŸini gÃ¶rselleÅŸtir
plot_training_history(history, "ğŸ¯ Basit Neural Network SonuÃ§larÄ±")

# Final performans metrikleri
final_train_acc = history.history['accuracy'][-1]
final_val_acc = history.history['val_accuracy'][-1]
final_train_loss = history.history['loss'][-1]
final_val_loss = history.history['val_loss'][-1]

print("ğŸ“Š FINAL PERFORMANS")
print("=" * 40)
print(f"ğŸ¯ EÄŸitim DoÄŸruluÄŸu: %{final_train_acc*100:.2f}")
print(f"ğŸ¯ DoÄŸrulama DoÄŸruluÄŸu: %{final_val_acc*100:.2f}")
print(f"ğŸ“‰ EÄŸitim KaybÄ±: {final_train_loss:.4f}")
print(f"ğŸ“‰ DoÄŸrulama KaybÄ±: {final_val_loss:.4f}")

# Overfitting kontrolÃ¼
accuracy_gap = abs(final_train_acc - final_val_acc)
if accuracy_gap > 0.15:
    print(f"âš ï¸ OVERFITTING RÄ°SKÄ° VAR (Fark: %{accuracy_gap*100:.2f})")
    print("   ğŸ’¡ Ã‡Ã¶zÃ¼m Ã¶nerileri: Dropout, daha az epoch, daha az parametre")
elif accuracy_gap > 0.05:
    print(f"âš¡ HAFÄ°F OVERFITTING (Fark: %{accuracy_gap*100:.2f})")
    print("   ğŸ’¡ Normal seviyede, endiÅŸelenmeyin")
else:
    print("âœ… MODEL DENGELÄ° GÃ–RÃœNÃœYOR")
    print("   ğŸ‰ EÄŸitim ve doÄŸrulama performansÄ± uyumlu")

"""# ğŸ“– PART 3: Aktivasyon FonksiyonlarÄ±nÄ±n KarÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±

Aktivasyon fonksiyonlarÄ±:Bu fonksiyonlar olmasaydÄ±, modelimiz ne kadar katmandan oluÅŸursa oluÅŸsun sadece dÃ¼z bir Ã§izgi Ã§izebilirdi. BalÄ±k gibi karmaÅŸÄ±k ÅŸekilleri Ã¶ÄŸrenmesi iÃ§in, modelin bu 'bÃ¼kÃ¼lme' yeteneÄŸine ihtiyacÄ± var.

- **Sigmoid:** 0â€“1 arasÄ± deÄŸer dÃ¶ndÃ¼rÃ¼r.
- **Tanh:** -1 ile 1 arasÄ± deÄŸer dÃ¶ndÃ¼rÃ¼r.
- **ReLU:** Negatifleri sÄ±fÄ±rlar, pozitifleri aynen geÃ§irir.
- **Leaky ReLU:** ReLUâ€™nun geliÅŸtirilmiÅŸ hali, negatiflere kÃ¼Ã§Ã¼k deÄŸerler bÄ±rakÄ±r.

AynÄ± mimariyi farklÄ± aktivasyonlarla Ã§alÄ±ÅŸtÄ±rÄ±p sonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±racaÄŸÄ±z.
"""

import time
from tensorflow.keras import backend as K

def create_activation_test_model(activation, input_shape, num_classes):
    """
    Belirli aktivasyon fonksiyonlu test modeli oluÅŸturur
    """
    model = models.Sequential([
        layers.Input(shape=input_shape),
        layers.Rescaling(1./255),
        layers.Flatten(),

        # Gizli katman - test edilecek aktivasyon
        layers.Dense(64, name="hidden_layer"),

        # Aktivasyon katmanÄ±nÄ± ayrÄ± ekle (Leaky ReLU iÃ§in)
        layers.LeakyReLU(alpha=0.1) if activation == 'leaky_relu'
        else layers.Activation(activation),

        # Ã‡Ä±kÄ±ÅŸ katmanÄ±
        layers.Dense(num_classes, activation='softmax', name="output")
    ], name=f"Model_{activation}")

    return model

# Test edilecek aktivasyon fonksiyonlarÄ±
activations_to_test = {
    'relu': 'ReLU',
    'sigmoid': 'Sigmoid',
    'tanh': 'Tanh',
    'leaky_relu': 'Leaky ReLU'
}

print("ğŸ§ª AKTÄ°VASYON FONKSÄ°YONU KARÅILAÅTIRMASI")
print("=" * 60)
print("ğŸ¯ Test parametreleri:")
print(f"   ğŸ“Š Model mimarisi: Flatten -> Dense(64) -> Dense({num_classes})")
print(f"   â° Epoch sayÄ±sÄ±: 3 (hÄ±zlÄ± karÅŸÄ±laÅŸtÄ±rma)")
print(f"   ğŸ² Batch size: {BATCH_SIZE}")
print()

# SonuÃ§larÄ± saklamak iÃ§in
activation_results = {}
activation_times = {}

print("ğŸš€ Testler baÅŸlÄ±yor...")
print("-" * 50)

total_start_time = time.time()

for i, (act_key, act_name) in enumerate(activations_to_test.items()):
    print(f"ğŸ”„ Test {i+1}/{len(activations_to_test)}: {act_name}")

    # Model oluÅŸtur
    test_model = create_activation_test_model(
        activation=act_key,
        input_shape=IMG_SIZE + (3,),
        num_classes=num_classes
    )

    # Modeli derle
    test_model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    # EÄŸitimi baÅŸlat
    start_time = time.time()

    history = test_model.fit(
        train_ds,
        epochs=3,  # HÄ±zlÄ± karÅŸÄ±laÅŸtÄ±rma iÃ§in az epoch
        validation_data=val_ds,
        verbose=0  # Sessiz mod
    )

    end_time = time.time()

    # SonuÃ§larÄ± kaydet
    activation_results[act_name] = history.history
    activation_times[act_name] = end_time - start_time

    # SonuÃ§larÄ± yazdÄ±r
    final_val_acc = history.history['val_accuracy'][-1]
    training_time = end_time - start_time

    print(f"   âœ… {act_name}: Val Acc = %{final_val_acc*100:.2f}, SÃ¼re = {training_time:.1f}s")

    # Memory temizliÄŸi
    del test_model
    K.clear_session()

total_end_time = time.time()
total_time = total_end_time - total_start_time

print("-" * 50)
print(f"ğŸ Toplam test sÃ¼resi: {total_time:.2f} saniye")

# KarÅŸÄ±laÅŸtÄ±rma grafiÄŸini oluÅŸtur
def plot_activation_comparison(results, times):
    """
    Aktivasyon fonksiyonu karÅŸÄ±laÅŸtÄ±rma grafikleri
    """
    fig = plt.figure(figsize=(18, 12))

    # 1. Validation Accuracy karÅŸÄ±laÅŸtÄ±rmasÄ±
    ax1 = plt.subplot(2, 3, 1)
    for act_name, hist in results.items():
        epochs = range(1, len(hist['val_accuracy']) + 1)
        plt.plot(epochs, hist['val_accuracy'],
                label=act_name, marker='o', linewidth=2)

    plt.title('ğŸ¯ DoÄŸrulama DoÄŸruluÄŸu KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.ylim(0, 1)

    # 2. Training Loss karÅŸÄ±laÅŸtÄ±rmasÄ±
    ax2 = plt.subplot(2, 3, 2)
    for act_name, hist in results.items():
        epochs = range(1, len(hist['loss']) + 1)
        plt.plot(epochs, hist['loss'],
                label=act_name, marker='s', linewidth=2)

    plt.title('ğŸ“‰ EÄŸitim KaybÄ± KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 3. Validation Loss karÅŸÄ±laÅŸtÄ±rmasÄ±
    ax3 = plt.subplot(2, 3, 3)
    for act_name, hist in results.items():
        epochs = range(1, len(hist['val_loss']) + 1)
        plt.plot(epochs, hist['val_loss'],
                label=act_name, marker='^', linewidth=2)

    plt.title('ğŸ“‰ DoÄŸrulama KaybÄ± KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Validation Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 4. Final accuracy bar chart
    ax4 = plt.subplot(2, 3, 4)
    final_accuracies = [hist['val_accuracy'][-1] for hist in results.values()]
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']

    bars = plt.bar(results.keys(), final_accuracies, color=colors, alpha=0.8)
    plt.title('ğŸ“Š Final DoÄŸrulama DoÄŸruluÄŸu', fontweight='bold')
    plt.ylabel('Accuracy')
    plt.ylim(0, 1)

    # Bar deÄŸerlerini gÃ¶ster
    for bar, acc in zip(bars, final_accuracies):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')

    plt.xticks(rotation=45)

    # 5. Training time karÅŸÄ±laÅŸtÄ±rmasÄ±
    ax5 = plt.subplot(2, 3, 5)
    training_times = list(times.values())
    colors_time = ['#FF9999', '#66B2FF', '#99FF99', '#FFB366']

    bars_time = plt.bar(times.keys(), training_times, color=colors_time, alpha=0.8)
    plt.title('â° EÄŸitim SÃ¼resi KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontweight='bold')
    plt.ylabel('SÃ¼re (saniye)')

    # Bar deÄŸerlerini gÃ¶ster
    for bar, time_val in zip(bars_time, training_times):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{time_val:.1f}s', ha='center', va='bottom', fontweight='bold')

    plt.xticks(rotation=45)

    # 6. Accuracy vs Time scatter plot
    ax6 = plt.subplot(2, 3, 6)
    scatter_colors = ['red', 'blue', 'green', 'orange']

    for i, (act_name, hist) in enumerate(results.items()):
        final_acc = hist['val_accuracy'][-1]
        time_taken = times[act_name]
        plt.scatter(time_taken, final_acc, s=200, alpha=0.7,
                   color=scatter_colors[i], label=act_name)
        plt.annotate(act_name, (time_taken, final_acc),
                    xytext=(5, 5), textcoords='offset points', fontsize=10)

    plt.title('âš¡ Performans vs HÄ±z', fontweight='bold')
    plt.xlabel('EÄŸitim SÃ¼resi (saniye)')
    plt.ylabel('Final Accuracy')
    plt.grid(True, alpha=0.3)
    plt.legend()

    plt.tight_layout()
    plt.suptitle('ğŸ§ª Aktivasyon Fonksiyonu KarÅŸÄ±laÅŸtÄ±rmasÄ±',
                 fontsize=16, fontweight='bold', y=1.02)
    plt.show()

# KarÅŸÄ±laÅŸtÄ±rma grafiklerini Ã§iz
plot_activation_comparison(activation_results, activation_times)

# En iyi performansÄ± belirle ve Ã¶zet Ã§Ä±kar
print("ğŸ“ˆ AKTIVASYON FONKSIYONU KARÅILAÅTIRMA SONUÃ‡LARI")
print("=" * 60)

best_accuracy = 0
best_activation = ""
fastest_activation = ""
fastest_time = float('inf')

for act_name, hist in activation_results.items():
    final_acc = hist['val_accuracy'][-1]
    time_taken = activation_times[act_name]

    print(f"ğŸ¯ {act_name}:")
    print(f"   ğŸ“Š Final Accuracy: %{final_acc*100:.2f}")
    print(f"   â° EÄŸitim SÃ¼resi: {time_taken:.2f} saniye")
    print(f"   ğŸ“ˆ Accuracy ArtÄ±ÅŸÄ±: %{(final_acc - hist['val_accuracy'][0])*100:.2f}")
    print()

    if final_acc > best_accuracy:
        best_accuracy = final_acc
        best_activation = act_name

    if time_taken < fastest_time:
        fastest_time = time_taken
        fastest_activation = act_name

print("ğŸ† Ã–ZET:")
print(f"   ğŸ¥‡ En Ä°yi Accuracy: {best_activation} (%{best_accuracy*100:.2f})")
print(f"   âš¡ En HÄ±zlÄ±: {fastest_activation} ({fastest_time:.2f}s)")

# Aktivasyon fonksiyonu Ã¶nerileri
print("\nğŸ’¡ AKTÄ°VASYON FONKSÄ°YONU REHBERÄ°:")
print("   ğŸš€ ReLU: Genel amaÃ§lÄ±, hÄ±zlÄ±, gradient problem yok")
print("   ğŸ“Š Sigmoid: Binary sÄ±nÄ±flandÄ±rma, gradient vanishing riski")
print("   ğŸ”„ Tanh: SÄ±fÄ±r merkezli, daha iyi gradient flow")
print("   ğŸ”§ Leaky ReLU: ReLU'nun geliÅŸmiÅŸ versiyonu, dying neuron problemi yok")

"""# ğŸ“– PART 4: Loss FonksiyonlarÄ± ve Optimizasyon
**Learning Rate**
Optimizasyoncunun hatalarÄ± dÃ¼zeltirken ne kadar bÃ¼yÃ¼k adÄ±mlar atacaÄŸÄ±nÄ± belirler. Ã‡ok bÃ¼yÃ¼k adÄ±m (yÃ¼ksek oran) hedefi ÅŸaÅŸÄ±rtÄ±r, Ã§ok kÃ¼Ã§Ã¼k adÄ±m (dÃ¼ÅŸÃ¼k oran) ise eÄŸitimi Ã§ok yavaÅŸlatÄ±r.

ğŸ“‰ **Loss FonksiyonlarÄ±**
Loss (KayÄ±p) fonksiyonu, modelin tahminlerinin gerÃ§ek deÄŸerlerden ne kadar uzak olduÄŸunu Ã¶lÃ§er. Model bu kaybÄ± minimize etmeye Ã§alÄ±ÅŸÄ±r.
ğŸ¯ Ana Loss FonksiyonlarÄ±:

Sparse Categorical Crossentropy

* Ã‡oklu sÄ±nÄ±f problemleri iÃ§in
* Etiketler integer format (0, 1, 2, ...)
Bizim projemizde kullanÄ±yoruz


Categorical Crossentropy

* Ã‡oklu sÄ±nÄ±f problemleri iÃ§in
* Etiketler one-hot encoded format


Binary Crossentropy

* Ä°kili sÄ±nÄ±flandÄ±rma iÃ§in
* Ã‡Ä±kÄ±ÅŸ 0 ile 1 arasÄ±nda


Mean Squared Error (MSE)

* Regresyon problemleri iÃ§in
* SÃ¼rekli deÄŸer tahmini

**Loss Fonksiyonu SeÃ§imi: Neden Sparse Categorical Crossentropy?**
Loss Fonksiyonu, modelin **"yaptÄ±ÄŸÄ± hata"**yÄ± Ã¶lÃ§er. Bu projede, modelimiz balÄ±ÄŸÄ± 7 farklÄ± tÃ¼rden birine sÄ±nÄ±flandÄ±rmaya Ã§alÄ±ÅŸÄ±yor. Bu nedenle, kullanacaÄŸÄ±mÄ±z Loss Fonksiyonu, Ã§ok sÄ±nÄ±flÄ± (multiclass) sÄ±nÄ±flandÄ±rma problemlerine uygun olmalÄ±dÄ±r.

Burada "Sparse Categorical Crossentropy" (Seyrek Kategorik Ã‡apraz Entropi) adÄ±nda biraz karmaÅŸÄ±k duran bir fonksiyon seÃ§iyoruz. Bu seÃ§imin ardÄ±nda yatan iki basit neden var:

1. Ã‡oklu SÄ±nÄ±flandÄ±rma (Categorical)
BalÄ±k tÃ¼rlerimiz (Black Sea Sprat, Bream vb.) birer kategoridir. Bu tÃ¼r kategorileri doÄŸru tahmin etmek iÃ§in Crossentropy'yi kullanÄ±rÄ±z. Crossentropy, modelin tahmin ettiÄŸi olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± ile gerÃ§ek etiket arasÄ±ndaki farkÄ± Ã¶lÃ§mekte en baÅŸarÄ±lÄ± fonksiyondur.

2. Etiket FormatÄ±mÄ±z (Sparse)
Bizim veri setimizde, balÄ±k tÃ¼rleri etiketlenirken genellikle ÅŸu format kullanÄ±lÄ±r:

GerÃ§ek Etiket: 0 (Black Sea Sprat)

GerÃ§ek Etiket: 1 (Bream)

...

GerÃ§ek Etiket: 6 (Shrimp)

Bu formatta, her bir balÄ±ÄŸÄ±n etiketine karÅŸÄ±lÄ±k gelen sadece bir tane tamsayÄ± (integer) vardÄ±r.

EÄŸer etiketlerimiz tamsayÄ± yerine, her biri iÃ§in ayrÄ± bir sÃ¼tun oluÅŸturulmuÅŸ, [0, 0, 1, 0, 0, 0, 0] gibi bir one-hot encoded formatÄ±nda olsaydÄ±, "Categorical Crossentropy" kullanÄ±rdÄ±k.

Ancak bizim etiketlerimiz tamsayÄ± olduÄŸu iÃ§in (daha seyrek bir temsil), Keras'ta Loss Fonksiyonu olarak Sparse Categorical Crossentropy'yi seÃ§eriz.
"""

def create_loss_comparison_model(loss_function, input_shape, num_classes):
    """
    Belirli loss fonksiyonlu model oluÅŸturur
    """
    model = models.Sequential([
        layers.Input(shape=input_shape),
        layers.Rescaling(1./255),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.3),  # Overfitting Ã¶nleme
        layers.Dense(num_classes, activation='softmax')
    ], name=f"LossTest_{loss_function}")

    return model

# Test edilecek loss fonksiyonlarÄ±
loss_functions = {
    'sparse_categorical_crossentropy': 'Sparse Categorical CE',
    'categorical_crossentropy': 'Categorical CE',
    'mean_squared_error': 'Mean Squared Error'
}

print("ğŸ“‰ LOSS FONKSÄ°YONU KARÅILAÅTIRMASI")
print("=" * 60)
print("âš ï¸  Not: Categorical CE ve MSE tam uyumlu olmayabilir (veri format farkÄ±)")
print()

loss_results = {}
loss_training_times = {}

for i, (loss_key, loss_name) in enumerate(loss_functions.items()):
    print(f"ğŸ”„ Test {i+1}/{len(loss_functions)}: {loss_name}")

    try:
        # Model oluÅŸtur
        model = create_loss_comparison_model(
            loss_function=loss_key,
            input_shape=IMG_SIZE + (3,),
            num_classes=num_classes
        )

        # Ã–zel compiler ayarlarÄ±
        if loss_key == 'categorical_crossentropy':
            # One-hot encoding gerekli, skip bu test
            print(f"   âš ï¸ {loss_name}: Veri format uyumsuzluÄŸu nedeniyle atlando")
            continue
        elif loss_key == 'mean_squared_error':
            # MSE regresyon iÃ§in tasarlandÄ±, sÄ±nÄ±flandÄ±rmada garip sonuÃ§lar verebilir
            print(f"   âš ï¸ {loss_name}: SÄ±nÄ±flandÄ±rma problemi iÃ§in uygun deÄŸil, atlando")
            continue

        # Normal compile
        model.compile(
            optimizer='adam',
            loss=loss_key,
            metrics=['accuracy']
        )

        # EÄŸitim
        start_time = time.time()
        history = model.fit(
            train_ds,
            epochs=3,
            validation_data=val_ds,
            verbose=0
        )
        end_time = time.time()

        # SonuÃ§larÄ± kaydet
        loss_results[loss_name] = history.history
        loss_training_times[loss_name] = end_time - start_time

        final_val_acc = history.history['val_accuracy'][-1]
        print(f"   âœ… {loss_name}: Val Acc = %{final_val_acc*100:.2f}")

        # Memory temizliÄŸi
        del model
        K.clear_session()

    except Exception as e:
        print(f"   âŒ {loss_name}: Hata - {str(e)[:50]}...")

print(f"\nâœ… Loss fonksiyonu testleri tamamlandÄ±!")

# Sadece Sparse Categorical CE ile detaylÄ± analiz
print("\nğŸ“Š SPARSE CATEGORICAL CROSSENTROPY ANALÄ°ZÄ°")
print("=" * 50)
print("Bu loss fonksiyonu neden balÄ±k sÄ±nÄ±flandÄ±rmasÄ± iÃ§in ideal:")
print("   ğŸ¯ Ã‡oklu sÄ±nÄ±f problemleri iÃ§in optimize edilmiÅŸ")
print("   ğŸ“Š Integer etiketlerle Ã§alÄ±ÅŸÄ±r (0, 1, 2, 3, ...)")
print("   âš¡ Hesaplama aÃ§Ä±sÄ±ndan verimli")
print("   ğŸ§  Softmax aktivasyonuyla mÃ¼kemmel uyum")

"""# ğŸ“– PART 5: Optimizasyon AlgoritmalarÄ±

EÄŸitim sÄ±rasÄ±nda modelin aÄŸÄ±rlÄ±klarÄ±nÄ± gÃ¼ncellemek iÃ§in **optimizer** kullanÄ±lÄ±r.  
BaÅŸlÄ±ca optimizasyon algoritmalarÄ±:  
- **SGD (Stochastic Gradient Descent)**: Klasik yÃ¶ntem, yavaÅŸ ama saÄŸlam.  
- **Adam:** En Ã§ok kullanÄ±lan, adaptif Ã¶ÄŸrenme oranÄ±.  
- **RMSprop:** Ã–zellikle RNN ve zaman serilerinde baÅŸarÄ±lÄ±.  

Åimdi aynÄ± modelde farklÄ± optimizerâ€™larÄ± deneyelim ve karÅŸÄ±laÅŸtÄ±ralÄ±m.
"""

def create_optimizer_test_model(optimizer, input_shape, num_classes):
    """
    Belirli optimizer'lÄ± model oluÅŸturur
    """
    model = models.Sequential([
        layers.Input(shape=input_shape),
        layers.Rescaling(1./255),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(num_classes, activation='softmax')
    ], name=f"OptimizerTest")

    return model

# Test edilecek optimizer'lar
optimizers_to_test = {
    'sgd': ('SGD', tf.keras.optimizers.SGD(learning_rate=0.01)),
    'adam': ('Adam', tf.keras.optimizers.Adam(learning_rate=0.001)),
    'rmsprop': ('RMSprop', tf.keras.optimizers.RMSprop(learning_rate=0.001)),
    'adagrad': ('Adagrad', tf.keras.optimizers.Adagrad(learning_rate=0.01))
}

print("âš™ï¸ OPTÄ°MÄ°ZER KARÅILAÅTIRMASI")
print("=" * 60)

print("ğŸ¯ Test edilecek optimizer'lar:")
for opt_key, (opt_name, opt_obj) in optimizers_to_test.items():
    print(f"   ğŸ“Œ {opt_name}: {opt_obj.get_config()['learning_rate']} learning rate")
print()

optimizer_results = {}
optimizer_times = {}

for i, (opt_key, (opt_name, optimizer)) in enumerate(optimizers_to_test.items()):
    print(f"ğŸ”„ Test {i+1}/{len(optimizers_to_test)}: {opt_name}")

    # Model oluÅŸtur
    model = create_optimizer_test_model(
        optimizer=optimizer,
        input_shape=IMG_SIZE + (3,),
        num_classes=num_classes
    )

    # Modeli derle
    model.compile(
        optimizer=optimizer,
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    # EÄŸitim
    start_time = time.time()
    history = model.fit(
        train_ds,
        epochs=4,  # Optimizer farkÄ±nÄ± gÃ¶rmek iÃ§in biraz daha fazla epoch
        validation_data=val_ds,
        verbose=0
    )
    end_time = time.time()

    # SonuÃ§larÄ± kaydet
    optimizer_results[opt_name] = history.history
    optimizer_times[opt_name] = end_time - start_time

    final_val_acc = history.history['val_accuracy'][-1]
    training_time = end_time - start_time
    print(f"   âœ… {opt_name}: Val Acc = %{final_val_acc*100:.2f}, SÃ¼re = {training_time:.1f}s")

    # Memory temizliÄŸi
    del model
    K.clear_session()

print("\nğŸ Optimizer testleri tamamlandÄ±!")

# Optimizer karÅŸÄ±laÅŸtÄ±rma grafiÄŸi
def plot_optimizer_comparison(results, times):
    """
    Optimizer karÅŸÄ±laÅŸtÄ±rma grafikleri oluÅŸturur
    """
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    # 1. Validation Accuracy
    ax1 = axes[0, 0]
    for opt_name, hist in results.items():
        epochs = range(1, len(hist['val_accuracy']) + 1)
        ax1.plot(epochs, hist['val_accuracy'],
                label=opt_name, marker='o', linewidth=2)

    ax1.set_title('ğŸ¯ DoÄŸrulama DoÄŸruluÄŸu', fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Validation Accuracy')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1)

    # 2. Training Loss
    ax2 = axes[0, 1]
    for opt_name, hist in results.items():
        epochs = range(1, len(hist['loss']) + 1)
        ax2.plot(epochs, hist['loss'],
                label=opt_name, marker='s', linewidth=2)

    ax2.set_title('ğŸ“‰ EÄŸitim KaybÄ±', fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Training Loss')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # 3. Convergence Speed (Loss azalma hÄ±zÄ±)
    ax3 = axes[1, 0]
    for opt_name, hist in results.items():
        loss_reduction = []
        initial_loss = hist['loss'][0]
        for loss in hist['loss']:
            reduction_percent = ((initial_loss - loss) / initial_loss) * 100
            loss_reduction.append(reduction_percent)

        epochs = range(1, len(loss_reduction) + 1)
        ax3.plot(epochs, loss_reduction,
                label=opt_name, marker='^', linewidth=2)

    ax3.set_title('âš¡ Konverjans HÄ±zÄ±', fontweight='bold')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Loss Azalma YÃ¼zdesi (%)')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # 4. Final Performance vs Time
    ax4 = axes[1, 1]
    final_accuracies = [hist['val_accuracy'][-1] for hist in results.values()]
    training_times = list(times.values())
    colors = ['red', 'blue', 'green', 'orange']

    for i, (opt_name, acc, time_val) in enumerate(zip(results.keys(), final_accuracies, training_times)):
        ax4.scatter(time_val, acc, s=200, alpha=0.7,
                   color=colors[i], label=opt_name)
        ax4.annotate(opt_name, (time_val, acc),
                    xytext=(5, 5), textcoords='offset points')

    ax4.set_title('ğŸ“Š Performans vs SÃ¼re', fontweight='bold')
    ax4.set_xlabel('EÄŸitim SÃ¼resi (saniye)')
    ax4.set_ylabel('Final Validation Accuracy')
    ax4.grid(True, alpha=0.3)
    ax4.legend()

    plt.tight_layout()
    plt.suptitle('âš™ï¸ Optimizer KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontsize=16, fontweight='bold', y=1.02)
    plt.show()

# Optimizer karÅŸÄ±laÅŸtÄ±rma grafiklerini Ã§iz
plot_optimizer_comparison(optimizer_results, optimizer_times)

# Optimizer analizi
print("ğŸ“ˆ OPTÄ°MÄ°ZER KARÅILAÅTIRMA ANALÄ°ZÄ°")
print("=" * 60)

best_optimizer = ""
best_performance = 0

for opt_name, hist in optimizer_results.items():
    final_acc = hist['val_accuracy'][-1]
    initial_loss = hist['loss'][0]
    final_loss = hist['loss'][-1]
    convergence_speed = ((initial_loss - final_loss) / initial_loss) * 100

    print(f"ğŸ¯ {opt_name}:")
    print(f"   ğŸ“Š Final Accuracy: %{final_acc*100:.2f}")
    print(f"   ğŸ“‰ Loss AzalmasÄ±: %{convergence_speed:.1f}")
    print(f"   â° EÄŸitim SÃ¼resi: {optimizer_times[opt_name]:.2f}s")
    print(f"   ğŸ’¡ Stabilite: {'KararlÄ±' if max(hist['val_accuracy']) - min(hist['val_accuracy']) < 0.1 else 'DalgalÄ±'}")
    print()

    if final_acc > best_performance:
        best_performance = final_acc
        best_optimizer = opt_name

print("ğŸ† SONUÃ‡ VE Ã–NERÄ°LER:")
print(f"   ğŸ¥‡ En Ä°yi Optimizer: {best_optimizer} (%{best_performance*100:.2f})")
print()
print("ğŸ’¡ OPTIMIZER REHBERÄ°:")
print("   ğŸš€ Adam: Genel amaÃ§lÄ±, adaptive learning rate, Ã§oÄŸu durumda iyi")
print("   ğŸ“ˆ SGD: Klasik, basit, bazen daha kararlÄ± sonuÃ§lar")
print("   âš¡ RMSprop: RNN ve sequence modellerde etkili")
print("   ğŸ“Š Adagrad: Sparse data iÃ§in iyi, learning rate azalÄ±r")

"""# ğŸ“– PART 5: Regularization Teknikleri (AÅŸÄ±rÄ± Ã–ÄŸrenmeyi Ã–nleme)
ğŸ›¡ï¸**Overfitting Nedir?**

Overfitting (AÅŸÄ±rÄ± Ã–ÄŸrenme), modelin eÄŸitim verilerini ezberleyip genel kurallarÄ± Ã¶ÄŸrenememesi durumudur.

ğŸš¨ Overfitting Belirtileri: Modelin sadece eÄŸitim verilerini ezberlemesi, ancak yeni (gÃ¶rmediÄŸi) balÄ±k gÃ¶rsellerini tanÄ±makta zorlanmasÄ±." En iyi benzetme: "SÄ±nav sorularÄ±nÄ± ezberleyen ama konuyu anlamayan Ã¶ÄŸrenci.

* EÄŸitim accuracy yÃ¼ksek, validation accuracy dÃ¼ÅŸÃ¼k
* Validation loss artmaya baÅŸlÄ±yor
* Model yeni verilerle baÅŸarÄ±sÄ±z

ğŸ›¡ï¸ Regularization Teknikleri:

* Dropout: Rastgele nÃ¶ronlarÄ± eÄŸitim sÄ±rasÄ±nda kapatÄ±r(EÄŸitim sÄ±rasÄ±nda nÃ¶ronlarÄ±n rastgele bir kÄ±smÄ±nÄ± 'uyutarak' modelin tek bir nÃ¶rona aÅŸÄ±rÄ± baÄŸÄ±mlÄ± olmasÄ±nÄ± engeller. Bu, takÄ±m Ã§alÄ±ÅŸmasÄ±na zorlamak gibidir.)
* Batch Normalization: Katman giriÅŸlerini normalize eder
* L1/L2 Regularization: AÄŸÄ±rlÄ±klarÄ± cezalandÄ±rÄ±r
* Early Stopping: Validation loss artmaya baÅŸlayÄ±nca durdurur
"""

def create_regularization_model(reg_type, input_shape, num_classes):
    """
    FarklÄ± regularization teknikleriyle model oluÅŸturur
    """
    models_dict = {
        'baseline': models.Sequential([
            layers.Input(shape=input_shape),
            layers.Rescaling(1./255),
            layers.Flatten(),
            layers.Dense(256, activation='relu'),
            layers.Dense(128, activation='relu'),
            layers.Dense(num_classes, activation='softmax')
        ], name="Baseline"),

        'dropout': models.Sequential([
            layers.Input(shape=input_shape),
            layers.Rescaling(1./255),
            layers.Flatten(),
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.5),  # %50 nÃ¶ron kapatÄ±lÄ±r
            layers.Dense(128, activation='relu'),
            layers.Dropout(0.3),  # %30 nÃ¶ron kapatÄ±lÄ±r
            layers.Dense(num_classes, activation='softmax')
        ], name="Dropout"),

        'batch_norm': models.Sequential([
            layers.Input(shape=input_shape),
            layers.Rescaling(1./255),
            layers.Flatten(),
            layers.Dense(256, activation='relu'),
            layers.BatchNormalization(),  # Normalizasyon
            layers.Dense(128, activation='relu'),
            layers.BatchNormalization(),
            layers.Dense(num_classes, activation='softmax')
        ], name="BatchNormalization"),

        'l2_reg': models.Sequential([
            layers.Input(shape=input_shape),
            layers.Rescaling(1./255),
            layers.Flatten(),
            layers.Dense(256, activation='relu',
                        kernel_regularizer=tf.keras.regularizers.l2(0.01)),  # L2 ceza
            layers.Dense(128, activation='relu',
                        kernel_regularizer=tf.keras.regularizers.l2(0.01)),
            layers.Dense(num_classes, activation='softmax')
        ], name="L2Regularization"),

        'combined': models.Sequential([
            layers.Input(shape=input_shape),
            layers.Rescaling(1./255),
            layers.Flatten(),
            layers.Dense(256, activation='relu',
                        kernel_regularizer=tf.keras.regularizers.l2(0.005)),
            layers.Dropout(0.4),
            layers.BatchNormalization(),
            layers.Dense(128, activation='relu',
                        kernel_regularizer=tf.keras.regularizers.l2(0.005)),
            layers.Dropout(0.2),
            layers.Dense(num_classes, activation='softmax')
        ], name="Combined")
    }

    return models_dict[reg_type]

print("ğŸ›¡ï¸ REGULARÄ°ZATÄ°ON TEKNÄ°KLERÄ° KARÅILAÅTIRMASI")
print("=" * 60)

regularization_techniques = {
    'baseline': 'Temel Model (Regularization Yok)',
    'dropout': 'Dropout',
    'batch_norm': 'Batch Normalization',
    'l2_reg': 'L2 Regularization',
    'combined': 'Kombine (Dropout + BatchNorm + L2)'
}

reg_results = {}
reg_times = {}

print("ğŸ¯ Test edilecek teknikler:")
for reg_key, reg_name in regularization_techniques.items():
    print(f"   ğŸ“Œ {reg_name}")
print()

for i, (reg_key, reg_name) in enumerate(regularization_techniques.items()):
    print(f"ğŸ”„ Test {i+1}/{len(regularization_techniques)}: {reg_name}")

    # Model oluÅŸtur
    model = create_regularization_model(
        reg_type=reg_key,
        input_shape=IMG_SIZE + (3,),
        num_classes=num_classes
    )

    # Modeli derle
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    # EÄŸitim (daha fazla epoch - overfitting'i gÃ¶zlemlemek iÃ§in)
    start_time = time.time()
    history = model.fit(
        train_ds,
        epochs=6,  # Overfitting gÃ¶zlemlemek iÃ§in daha fazla epoch
        validation_data=val_ds,
        verbose=0
    )
    end_time = time.time()

    # SonuÃ§larÄ± kaydet
    reg_results[reg_name] = history.history
    reg_times[reg_name] = end_time - start_time

    # Overfitting analizi
    final_train_acc = history.history['accuracy'][-1]
    final_val_acc = history.history['val_accuracy'][-1]
    overfitting_gap = final_train_acc - final_val_acc

    print(f"   ğŸ“Š Train Acc: %{final_train_acc*100:.2f}")
    print(f"   ğŸ¯ Val Acc: %{final_val_acc*100:.2f}")
    print(f"   ğŸ“‰ Overfitting Gap: %{overfitting_gap*100:.2f}")

    if overfitting_gap > 0.15:
        print(f"   ğŸš¨ YÃ¼ksek overfitting riski!")
    elif overfitting_gap > 0.05:
        print(f"   âš ï¸ Hafif overfitting")
    else:
        print(f"   âœ… Dengeli model")
    print()

    # Memory temizliÄŸi
    del model
    K.clear_session()

print("ğŸ Regularization testleri tamamlandÄ±!")

# Regularization karÅŸÄ±laÅŸtÄ±rma grafiÄŸi
def plot_regularization_comparison(results):
    """
    Regularization tekniklerini karÅŸÄ±laÅŸtÄ±ran detaylÄ± grafikler
    """
    fig, axes = plt.subplots(3, 2, figsize=(16, 18))

    # 1. Training vs Validation Accuracy
    ax1 = axes[0, 0]
    for reg_name, hist in results.items():
        epochs = range(1, len(hist['accuracy']) + 1)
        ax1.plot(epochs, hist['accuracy'], '--', alpha=0.7, label=f'{reg_name} (Train)')
        ax1.plot(epochs, hist['val_accuracy'], '-', linewidth=2, label=f'{reg_name} (Val)')

    ax1.set_title('ğŸ¯ EÄŸitim vs DoÄŸrulama DoÄŸruluÄŸu', fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy')
    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    ax1.grid(True, alpha=0.3)

    # 2. Overfitting Gap Analysis
    ax2 = axes[0, 1]
    overfitting_gaps = []
    reg_names = []

    for reg_name, hist in results.items():
        final_train = hist['accuracy'][-1]
        final_val = hist['val_accuracy'][-1]
        gap = final_train - final_val
        overfitting_gaps.append(gap)
        reg_names.append(reg_name.replace(' (', '\n('))  # Ä°sim kÄ±rma

    colors = ['red' if gap > 0.15 else 'orange' if gap > 0.05 else 'green'
              for gap in overfitting_gaps]

    bars = ax2.bar(range(len(reg_names)), overfitting_gaps, color=colors, alpha=0.7)
    ax2.set_title('ğŸ“‰ Overfitting Analizi (Train - Val Acc)', fontweight='bold')
    ax2.set_ylabel('Accuracy FarkÄ±')
    ax2.set_xticks(range(len(reg_names)))
    ax2.set_xticklabels(reg_names, rotation=45, ha='right')
    ax2.axhline(y=0.05, color='orange', linestyle='--', alpha=0.7, label='Hafif Overfitting')
    ax2.axhline(y=0.15, color='red', linestyle='--', alpha=0.7, label='YÃ¼ksek Overfitting')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # Bar deÄŸerlerini gÃ¶ster
    for bar, gap in zip(bars, overfitting_gaps):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                f'{gap:.3f}', ha='center', va='bottom', fontweight='bold')

    # 3. Loss Comparison
    ax3 = axes[1, 0]
    for reg_name, hist in results.items():
        epochs = range(1, len(hist['loss']) + 1)
        ax3.plot(epochs, hist['loss'], '--', alpha=0.7)
        ax3.plot(epochs, hist['val_loss'], '-', linewidth=2, label=reg_name)

    ax3.set_title('ğŸ“‰ KayÄ±p Fonksiyonu KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontweight='bold')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Loss')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # 4. Final Performance Bar Chart
    ax4 = axes[1, 1]
    final_val_accs = [hist['val_accuracy'][-1] for hist in results.values()]
    colors_perf = plt.cm.viridis(np.linspace(0, 1, len(final_val_accs)))

    bars_perf = ax4.bar(range(len(reg_names)), final_val_accs, color=colors_perf, alpha=0.8)
    ax4.set_title('ğŸ“Š Final DoÄŸrulama PerformansÄ±', fontweight='bold')
    ax4.set_ylabel('Validation Accuracy')
    ax4.set_xticks(range(len(reg_names)))
    ax4.set_xticklabels(reg_names, rotation=45, ha='right')
    ax4.set_ylim(0, 1)

    # Bar deÄŸerlerini gÃ¶ster
    for bar, acc in zip(bars_perf, final_val_accs):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')
    ax4.grid(True, alpha=0.3)

    # 5. Training Stability (Validation accuracy variance)
    ax5 = axes[2, 0]
    stability_scores = []
    for reg_name, hist in results.items():
        val_acc_variance = np.var(hist['val_accuracy'])
        stability_scores.append(val_acc_variance)

    colors_stab = ['green' if score < 0.001 else 'orange' if score < 0.005 else 'red'
                   for score in stability_scores]

    bars_stab = ax5.bar(range(len(reg_names)), stability_scores, color=colors_stab, alpha=0.7)
    ax5.set_title('ğŸ“ˆ EÄŸitim KararlÄ±lÄ±ÄŸÄ± (Val Acc VaryansÄ±)', fontweight='bold')
    ax5.set_ylabel('Varyans')
    ax5.set_xticks(range(len(reg_names)))
    ax5.set_xticklabels(reg_names, rotation=45, ha='right')

    # Bar deÄŸerlerini gÃ¶ster
    for bar, score in zip(bars_stab, stability_scores):
        height = bar.get_height()
        ax5.text(bar.get_x() + bar.get_width()/2., height + height*0.05,
                f'{score:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9)
    ax5.grid(True, alpha=0.3)

    # 6. Learning Curves Comparison (last 3 epochs trend)
    ax6 = axes[2, 1]
    improvement_rates = []
    for reg_name, hist in results.items():
        # Son 3 epoch'taki val_accuracy artÄ±ÅŸ oranÄ±
        recent_accs = hist['val_accuracy'][-3:]
        if len(recent_accs) >= 2:
            improvement = (recent_accs[-1] - recent_accs[0]) / len(recent_accs)
        else:
            improvement = 0
        improvement_rates.append(improvement)

    colors_imp = ['green' if rate > 0.01 else 'orange' if rate > -0.01 else 'red'
                  for rate in improvement_rates]

    bars_imp = ax6.bar(range(len(reg_names)), improvement_rates, color=colors_imp, alpha=0.7)
    ax6.set_title('ğŸ“ˆ Son DÃ¶nem Ä°yileÅŸtirme Trendi', fontweight='bold')
    ax6.set_ylabel('Accuracy ArtÄ±ÅŸ OranÄ±')
    ax6.set_xticks(range(len(reg_names)))
    ax6.set_xticklabels(reg_names, rotation=45, ha='right')
    ax6.axhline(y=0, color='black', linestyle='-', alpha=0.3)

    # Bar deÄŸerlerini gÃ¶ster
    for bar, rate in zip(bars_imp, improvement_rates):
        height = bar.get_height()
        y_pos = height + 0.002 if height >= 0 else height - 0.002
        ax6.text(bar.get_x() + bar.get_width()/2., y_pos,
                f'{rate:.3f}', ha='center', va='bottom' if height >= 0 else 'top',
                fontweight='bold', fontsize=9)
    ax6.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.suptitle('ğŸ›¡ï¸ Regularization Teknikleri KarÅŸÄ±laÅŸtÄ±rmasÄ±',
                 fontsize=16, fontweight='bold', y=0.98)
    plt.show()

# Regularization karÅŸÄ±laÅŸtÄ±rma grafiklerini Ã§iz
plot_regularization_comparison(reg_results)

# DetaylÄ± regularization analizi
print("ğŸ“Š REGULARÄ°ZATÄ°ON TEKNÄ°K ANALÄ°ZÄ°")
print("=" * 60)

best_reg_technique = ""
best_reg_score = 0
most_stable = ""
lowest_variance = float('inf')

for reg_name, hist in reg_results.items():
    # Performans metrikleri
    final_train_acc = hist['accuracy'][-1]
    final_val_acc = hist['val_accuracy'][-1]
    overfitting_gap = final_train_acc - final_val_acc

    # KararlÄ±lÄ±k metrikleri
    val_acc_variance = np.var(hist['val_accuracy'])
    val_acc_std = np.std(hist['val_accuracy'])

    # Ä°yileÅŸtirme trendi
    recent_val_accs = hist['val_accuracy'][-3:]
    improvement_trend = (recent_val_accs[-1] - recent_val_accs[0]) / len(recent_val_accs) if len(recent_val_accs) >= 2 else 0

    print(f"ğŸ¯ {reg_name}:")
    print(f"   ğŸ“Š Final Val Accuracy: %{final_val_acc*100:.2f}")
    print(f"   ğŸ“‰ Overfitting Gap: %{overfitting_gap*100:.2f}")
    print(f"   ğŸ“ˆ KararlÄ±lÄ±k (Std Dev): {val_acc_std:.4f}")
    print(f"   ğŸ”„ Son DÃ¶nem Trend: {improvement_trend:+.4f}")

    # Overfitting durumu
    if overfitting_gap > 0.15:
        print(f"   ğŸš¨ Durum: YÃœKSEK OVERFÄ°TTÄ°NG")
    elif overfitting_gap > 0.05:
        print(f"   âš ï¸ Durum: HAFÄ°F OVERFÄ°TTÄ°NG")
    else:
        print(f"   âœ… Durum: DENGELÄ° MODEL")

    # Genel deÄŸerlendirme skoru (performans + kararlÄ±lÄ±k)
    balance_score = final_val_acc - (overfitting_gap * 0.5) - (val_acc_variance * 10)
    print(f"   ğŸ† Denge Skoru: {balance_score:.3f}")
    print()

    # En iyi tekniÄŸi belirle
    if balance_score > best_reg_score:
        best_reg_score = balance_score
        best_reg_technique = reg_name

    # En kararlÄ± tekniÄŸi belirle
    if val_acc_variance < lowest_variance:
        lowest_variance = val_acc_variance
        most_stable = reg_name

print("ğŸ† REGULARIZATION SONUÃ‡LARI:")
print(f"   ğŸ¥‡ En Dengeli Teknik: {best_reg_technique} (Skor: {best_reg_score:.3f})")
print(f"   âš–ï¸ En KararlÄ± Teknik: {most_stable} (Varyans: {lowest_variance:.4f})")
print()

print("ğŸ’¡ REGULARÄ°ZATÄ°ON REHBERÄ°:")
print("   ğŸ¯ Dropout: Basit ve etkili, %20-50 arasÄ±nda kullan")
print("   ğŸ“Š Batch Norm: EÄŸitimi hÄ±zlandÄ±rÄ±r, gradient flow'u iyileÅŸtirir")
print("   âš–ï¸ L2 Reg: AÄŸÄ±rlÄ±klarÄ± kÃ¼Ã§Ã¼k tutar, 0.001-0.01 arasÄ± dene")
print("   ğŸ”„ Early Stopping: En gÃ¼venli yÃ¶ntem, validation loss'u izle")
print("   ğŸ›¡ï¸ Kombine: Birden fazla tekniÄŸi birleÅŸtir, daha gÃ¼Ã§lÃ¼ koruma")

"""# ğŸ“– PART 7: CNN KatmanlarÄ± (Conv2D ve Pooling)

ğŸ–¼ï¸ Convolutional Neural Networks (CNN)
CNN, gÃ¶rÃ¼ntÃ¼ iÅŸleme iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸ neural network tÃ¼rÃ¼dÃ¼r. Ä°nsan gÃ¶rsel korteksinden ilham alÄ±r. Her bir filtre (kernel), bir bÃ¼yÃ¼teÃ§ gibidir. GÃ¶rseldeki kenarlarÄ±, kÃ¶ÅŸeleri veya balÄ±ÄŸÄ±n pullarÄ± gibi kÃ¼Ã§Ã¼k desenleri arar. Bu sayede model, gÃ¶rsellerin Ã¶zelliklerini otomatik olarak Ã¶ÄŸrenir.

ğŸ§  **CNN'in AvantajlarÄ±:**

Uzamsal yapÄ±yÄ± korur (piksel komÅŸuluklarÄ±)
Az parametre (aÄŸÄ±rlÄ±k paylaÅŸÄ±mÄ±)
Translation invariant (nesne konumu fark etmez)
Hierarchical feature learning (kenar â†’ doku â†’ ÅŸekil â†’ nesne)

ğŸ” **CNN Katman TÃ¼rleri**:

* Conv2D (KonvolÃ¼syon)

GÃ¶rÃ¼ntÃ¼ Ã¼zerinde filtreler (kernels) gezdirir
Kenar, kÃ¶ÅŸe, doku gibi Ã¶zellikleri Ã§Ä±karÄ±r
Parametre: filter sayÄ±sÄ±, kernel boyutu, stride, padding


* MaxPooling2D

Ã–zellik haritasÄ±nÄ±n boyutunu kÃ¼Ã§Ã¼ltÃ¼r
En bÃ¼yÃ¼k deÄŸeri seÃ§er (down-sampling)
Hesaplama yÃ¼kÃ¼nÃ¼ azaltÄ±r, invariance saÄŸlar


* AveragePooling2D

Ortalama deÄŸeri alÄ±r
MaxPooling'e alternatif
"""

def create_cnn_model(cnn_type, input_shape, num_classes):
    """
    FarklÄ± CNN mimarileri oluÅŸturur
    """
    models_dict = {
        'simple_cnn': models.Sequential([
            layers.Input(shape=input_shape),
            layers.Rescaling(1./255),

            # Ä°lk konvolÃ¼syon bloÄŸu
            layers.Conv2D(32, (3, 3), activation='relu', name='conv1'),
            layers.MaxPooling2D((2, 2), name='pool1'),

            # Ä°kinci konvolÃ¼syon bloÄŸu
            layers.Conv2D(64, (3, 3), activation='relu', name='conv2'),
            layers.MaxPooling2D((2, 2), name='pool2'),

            # SÄ±nÄ±flandÄ±rma katmanlarÄ±
            layers.Flatten(),
            layers.Dense(64, activation='relu', name='fc1'),
            layers.Dense(num_classes, activation='softmax', name='output')
        ], name="SimpleCNN"),

        'deep_cnn': models.Sequential([
            layers.Input(shape=input_shape),
            layers.Rescaling(1./255),

            # Birinci blok
            layers.Conv2D(32, (3, 3), activation='relu'),
            layers.Conv2D(32, (3, 3), activation='relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),

            # Ä°kinci blok
            layers.Conv2D(64, (3, 3), activation='relu'),
            layers.Conv2D(64, (3, 3), activation='relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),

            # ÃœÃ§Ã¼ncÃ¼ blok
            layers.Conv2D(128, (3, 3), activation='relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),

            # SÄ±nÄ±flandÄ±rma
            layers.Flatten(),
            layers.Dense(128, activation='relu'),
            layers.Dropout(0.5),
            layers.Dense(num_classes, activation='softmax')
        ], name="DeepCNN"),

        'vgg_style': models.Sequential([
            layers.Input(shape=input_shape),
            layers.Rescaling(1./255),

            # VGG-style bloklar
            # Blok 1
            layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
            layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
            layers.MaxPooling2D((2, 2)),

            # Blok 2
            layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
            layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
            layers.MaxPooling2D((2, 2)),

            # Blok 3
            layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
            layers.MaxPooling2D((2, 2)),

            # Classifier
            layers.Flatten(),
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.5),
            layers.Dense(num_classes, activation='softmax')
        ], name="VGGStyle"),

        'residual_cnn': models.Sequential([
            layers.Input(shape=input_shape),
            layers.Rescaling(1./255),

            # Ä°lk konvolÃ¼syon
            layers.Conv2D(64, (7, 7), strides=2, padding='same', activation='relu'),
            layers.MaxPooling2D((3, 3), strides=2, padding='same'),

            # Residual benzeri bloklar (basitleÅŸtirilmiÅŸ)
            layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
            layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
            layers.MaxPooling2D((2, 2)),

            layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
            layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
            layers.MaxPooling2D((2, 2)),

            # Global average pooling (FC yerine)
            layers.GlobalAveragePooling2D(),
            layers.Dense(num_classes, activation='softmax')
        ], name="ResidualCNN")
    }

    return models_dict[cnn_type]

print("ğŸ–¼ï¸ CNN MÄ°MARÄ°LERÄ° KARÅILAÅTIRMASI")
print("=" * 60)

cnn_architectures = {
    'simple_cnn': 'Basit CNN (2 Conv Blok)',
    'deep_cnn': 'Derin CNN (3 Conv Blok + Dropout)',
    'vgg_style': 'VGG TarzÄ± (Ã‡ift Conv + Padding)',
    'residual_cnn': 'Residual CNN (Global Pooling)'
}

print("ğŸ—ï¸ Test edilecek CNN mimarileri:")
for cnn_key, cnn_name in cnn_architectures.items():
    print(f"   ğŸ“Œ {cnn_name}")
print()

cnn_results = {}
cnn_times = {}
cnn_params = {}

for i, (cnn_key, cnn_name) in enumerate(cnn_architectures.items()):
    print(f"ğŸ”„ Test {i+1}/{len(cnn_architectures)}: {cnn_name}")

    # Model oluÅŸtur
    model = create_cnn_model(
        cnn_type=cnn_key,
        input_shape=IMG_SIZE + (3,),
        num_classes=num_classes
    )

    # Model bilgilerini kaydet
    total_params = model.count_params()
    cnn_params[cnn_name] = total_params
    print(f"   ğŸ“Š Toplam parametre sayÄ±sÄ±: {total_params:,}")

    # Modeli derle
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    # EÄŸitim
    start_time = time.time()
    history = model.fit(
        train_ds,
        epochs=5,  # CNN'ler daha yavaÅŸ, az epoch
        validation_data=val_ds,
        verbose=0
    )
    end_time = time.time()

    # SonuÃ§larÄ± kaydet
    cnn_results[cnn_name] = history.history
    cnn_times[cnn_name] = end_time - start_time

    final_val_acc = history.history['val_accuracy'][-1]
    training_time = end_time - start_time

    print(f"   âœ… Val Accuracy: %{final_val_acc*100:.2f}")
    print(f"   â° EÄŸitim sÃ¼resi: {training_time:.1f} saniye")
    print(f"   âš¡ Saniye/Epoch: {training_time/5:.1f}s")
    print()

    # Memory temizliÄŸi
    del model
    K.clear_session()

print("ğŸ CNN mimarisi testleri tamamlandÄ±!")

# CNN karÅŸÄ±laÅŸtÄ±rma grafiÄŸi
def plot_cnn_comparison(results, times, params):
    """
    CNN mimarilerini karÅŸÄ±laÅŸtÄ±ran kapsamlÄ± grafikler
    """
    fig, axes = plt.subplots(3, 2, figsize=(16, 18))

    # 1. Validation Accuracy Progression
    ax1 = axes[0, 0]
    for cnn_name, hist in results.items():
        epochs = range(1, len(hist['val_accuracy']) + 1)
        ax1.plot(epochs, hist['val_accuracy'],
                label=cnn_name, marker='o', linewidth=2)

    ax1.set_title('ğŸ¯ DoÄŸrulama DoÄŸruluÄŸu GeliÅŸimi', fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Validation Accuracy')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1)

    # 2. Training vs Validation Loss
    ax2 = axes[0, 1]
    for cnn_name, hist in results.items():
        epochs = range(1, len(hist['loss']) + 1)
        ax2.plot(epochs, hist['loss'], '--', alpha=0.7, label=f'{cnn_name} (Train)')
        ax2.plot(epochs, hist['val_loss'], '-', linewidth=2, label=f'{cnn_name} (Val)')

    ax2.set_title('ğŸ“‰ EÄŸitim vs DoÄŸrulama KaybÄ±', fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Loss')
    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    ax2.grid(True, alpha=0.3)

    # 3. Parameter Count vs Performance
    ax3 = axes[1, 0]
    final_accuracies = [hist['val_accuracy'][-1] for hist in results.values()]
    param_counts = list(params.values())
    colors = ['red', 'blue', 'green', 'orange']

    for i, (name, acc, param_count) in enumerate(zip(results.keys(), final_accuracies, param_counts)):
        ax3.scatter(param_count, acc, s=200, alpha=0.7,
                   color=colors[i], label=name)
        ax3.annotate(name.replace(' (', '\n('), (param_count, acc),
                    xytext=(5, 5), textcoords='offset points', fontsize=9)

    ax3.set_title('ğŸ“Š Parametre SayÄ±sÄ± vs Performans', fontweight='bold')
    ax3.set_xlabel('Toplam Parametre SayÄ±sÄ±')
    ax3.set_ylabel('Final Validation Accuracy')
    ax3.grid(True, alpha=0.3)
    ax3.legend()

    # 4. Training Time vs Performance
    ax4 = axes[1, 1]
    training_times = list(times.values())

    for i, (name, acc, time_val) in enumerate(zip(results.keys(), final_accuracies, training_times)):
        ax4.scatter(time_val, acc, s=200, alpha=0.7,
                   color=colors[i], label=name)
        ax4.annotate(name.replace(' (', '\n('), (time_val, acc),
                    xytext=(5, 5), textcoords='offset points', fontsize=9)

    ax4.set_title('â° EÄŸitim SÃ¼resi vs Performans', fontweight='bold')
    ax4.set_xlabel('EÄŸitim SÃ¼resi (saniye)')
    ax4.set_ylabel('Final Validation Accuracy')
    ax4.grid(True, alpha=0.3)
    ax4.legend()

    # 5. Architecture Efficiency (Accuracy per Parameter)
    ax5 = axes[2, 0]
    efficiency_scores = [acc / (param_count / 1000) for acc, param_count in zip(final_accuracies, param_counts)]
    colors_eff = plt.cm.viridis(np.linspace(0, 1, len(efficiency_scores)))

    bars = ax5.bar(range(len(results)), efficiency_scores, color=colors_eff, alpha=0.8)
    ax5.set_title('âš¡ Mimari Verimlilik (Acc/1K Param)', fontweight='bold')
    ax5.set_ylabel('Verimlilik Skoru')
    ax5.set_xticks(range(len(results)))
    ax5.set_xticklabels([name.replace(' (', '\n(') for name in results.keys()], rotation=45, ha='right')

    # Bar deÄŸerlerini gÃ¶ster
    for bar, score in zip(bars, efficiency_scores):
        height = bar.get_height()
        ax5.text(bar.get_x() + bar.get_width()/2., height + height*0.05,
                f'{score:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)
    ax5.grid(True, alpha=0.3)

    # 6. Learning Speed Analysis (First epoch improvement)
    ax6 = axes[2, 1]
    learning_speeds = []
    for cnn_name, hist in results.items():
        if len(hist['val_accuracy']) >= 2:
            first_epoch_improvement = hist['val_accuracy'][1] - hist['val_accuracy'][0]
        else:
            first_epoch_improvement = 0
        learning_speeds.append(first_epoch_improvement)

    colors_speed = ['green' if speed > 0.1 else 'orange' if speed > 0.05 else 'red'
                    for speed in learning_speeds]

    bars_speed = ax6.bar(range(len(results)), learning_speeds, color=colors_speed, alpha=0.7)
    ax6.set_title('ğŸš€ Ã–ÄŸrenme HÄ±zÄ± (1. Epoch Ä°yileÅŸtirme)', fontweight='bold')
    ax6.set_ylabel('Accuracy ArtÄ±ÅŸÄ±')
    ax6.set_xticks(range(len(results)))
    ax6.set_xticklabels([name.replace(' (', '\n(') for name in results.keys()], rotation=45, ha='right')

    # Bar deÄŸerlerini gÃ¶ster
    for bar, speed in zip(bars_speed, learning_speeds):
        height = bar.get_height()
        y_pos = height + 0.005 if height >= 0 else height - 0.005
        ax6.text(bar.get_x() + bar.get_width()/2., y_pos,
                f'{speed:.3f}', ha='center', va='bottom' if height >= 0 else 'top',
                fontweight='bold', fontsize=10)
    ax6.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax6.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.suptitle('ğŸ–¼ï¸ CNN Mimarilerinin KarÅŸÄ±laÅŸtÄ±rmasÄ±',
                 fontsize=16, fontweight='bold', y=0.98)
    plt.show()

# CNN karÅŸÄ±laÅŸtÄ±rma grafiklerini Ã§iz
plot_cnn_comparison(cnn_results, cnn_times, cnn_params)

# DetaylÄ± CNN analizi
print("ğŸ“Š CNN MÄ°MARÄ° ANALÄ°ZÄ°")
print("=" * 60)

best_cnn = ""
best_cnn_performance = 0
most_efficient = ""
best_efficiency = 0

for cnn_name, hist in cnn_results.items():
    final_val_acc = hist['val_accuracy'][-1]
    param_count = cnn_params[cnn_name]
    training_time = cnn_times[cnn_name]

    # Verimlilik hesaplama
    efficiency = final_val_acc / (param_count / 1000)  # Accuracy per 1K parameters
    time_efficiency = final_val_acc / (training_time / 60)  # Accuracy per minute

    # Learning speed
    learning_speed = hist['val_accuracy'][1] - hist['val_accuracy'][0] if len(hist['val_accuracy']) >= 2 else 0

    print(f"ğŸ–¼ï¸ {cnn_name}:")
    print(f"   ğŸ“Š Final Val Accuracy: %{final_val_acc*100:.2f}")
    print(f"   ğŸ§® Toplam parametre: {param_count:,}")
    print(f"   â° EÄŸitim sÃ¼resi: {training_time:.1f} saniye")
    print(f"   âš¡ Parametre verimliliÄŸi: {efficiency:.3f}")
    print(f"   ğŸš€ Zaman verimliliÄŸi: {time_efficiency:.3f}")
    print(f"   ğŸ“ˆ Ä°lk epoch Ã¶ÄŸrenme: %{learning_speed*100:.2f}")

    # Memory usage estimation (rough)
    estimated_memory_mb = param_count * 4 / (1024 * 1024)  # 4 bytes per param
    print(f"   ğŸ’¾ Tahmini memory: {estimated_memory_mb:.1f} MB")
    print()

    if final_val_acc > best_cnn_performance:
        best_cnn_performance = final_val_acc
        best_cnn = cnn_name

    if efficiency > best_efficiency:
        best_efficiency = efficiency
        most_efficient = cnn_name

print("ğŸ† CNN MÄ°MARÄ° SONUÃ‡LARI:")
print(f"   ğŸ¥‡ En Ä°yi Performans: {best_cnn} (%{best_cnn_performance*100:.2f})")
print(f"   âš¡ En Verimli Mimari: {most_efficient} ({best_efficiency:.3f})")
print()

print("ğŸ’¡ CNN MÄ°MARÄ° REHBERÄ°:")
print("   ğŸ—ï¸ Basit CNN: Az veri iÃ§in, hÄ±zlÄ± prototype")
print("   ğŸ¢ Derin CNN: Daha karmaÅŸÄ±k Ã¶zellikler, dropout ile overfitting kontrolÃ¼")
print("   ğŸ›ï¸ VGG TarzÄ±: Klasik mimari, gÃ¼venilir sonuÃ§lar")
print("   ğŸ”„ Residual: Ã‡ok derin aÄŸlar iÃ§in, gradient flow problemi Ã§Ã¶zÃ¼mÃ¼")
print("   ğŸ“ Global Pooling: FC katmanlarÄ±n yerine, daha az parametre")

"""# ğŸ“– PART 7: Transfer Learning (Ã–nceden EÄŸitilmiÅŸ Modeller)
ğŸš€ Transfer Learning Nedir?
Transfer Learning, Ã¶nceden bÃ¼yÃ¼k veri setlerinde (ImageNet gibi) eÄŸitilmiÅŸ modellerin bilgisini yeni gÃ¶revlerde kullanma tekniÄŸidir.
BaÅŸka bir problemde (Ã¶rneÄŸin binlerce genel gÃ¶rseli tanÄ±mada) eÄŸitilmiÅŸ bir 'uzman' modeli alÄ±p, sadece son birkaÃ§ katmanÄ±nÄ± bizim balÄ±k problemimize gÃ¶re ayarlÄ±yoruz. Bu, sÄ±fÄ±rdan eÄŸitimden Ã§ok daha hÄ±zlÄ± ve etkilidir.

ğŸ¯ AvantajlarÄ±:

- HÄ±zlÄ± eÄŸitim (birkaÃ§ epoch)
- Az veri ile yÃ¼ksek performans
- Daha iyi Ã¶zellik Ã§Ä±karma
- Hesaplama maliyeti dÃ¼ÅŸÃ¼k

ğŸ”„ Transfer Learning Stratejileri:

* Feature Extraction: Base model dondurulur, sadece classifier eÄŸitilir. UzmanÄ±n (Ã¶nceden eÄŸitilmiÅŸ model) temel gÃ¶rsel tanÄ±ma bilgisini koruma altÄ±na alÄ±yoruz ki, kÃ¼Ã§Ã¼k balÄ±k veri setimizle bu bÃ¼yÃ¼k bilgiyi yanlÄ±ÅŸlÄ±kla bozmayalÄ±m.

* Fine-Tuning: Base model'in son katmanlarÄ± da eÄŸitime dahil edilir

* Full Training: TÃ¼m model yeniden eÄŸitilir (Ã§ok veri gerekir)
"""

def create_transfer_model(base_model_name, input_shape, num_classes, strategy='feature_extraction'):
    """
    Transfer learning modelleri oluÅŸturur
    """
    # Base model seÃ§imi
    if base_model_name == 'mobilenetv2':
        base_model = tf.keras.applications.MobileNetV2(
            input_shape=input_shape,
            include_top=False,  # Son sÄ±nÄ±flandÄ±rma katmanÄ±nÄ± dahil etme
            weights='imagenet'  # ImageNet aÄŸÄ±rlÄ±klarÄ±nÄ± kullan
        )
        preprocess_func = tf.keras.applications.mobilenet_v2.preprocess_input
    elif base_model_name == 'vgg16':
        base_model = tf.keras.applications.VGG16(
            input_shape=input_shape,
            include_top=False,
            weights='imagenet'
        )
        preprocess_func = tf.keras.applications.vgg16.preprocess_input
    elif base_model_name == 'resnet50':
        base_model = tf.keras.applications.ResNet50(
            input_shape=input_shape,
            include_top=False,
            weights='imagenet'
        )
        preprocess_func = tf.keras.applications.resnet50.preprocess_input
    elif base_model_name == 'efficientnetb0':
        base_model = tf.keras.applications.EfficientNetB0(
            input_shape=input_shape,
            include_top=False,
            weights='imagenet'
        )
        preprocess_func = tf.keras.applications.efficientnet.preprocess_input

    # Strategy'ye gÃ¶re base model ayarlarÄ±
    if strategy == 'feature_extraction':
        base_model.trainable = False  # Base model'i dondur
    elif strategy == 'fine_tuning':
        base_model.trainable = True   # Base model'i eÄŸitilebilir yap
        # Son birkaÃ§ katmanÄ± eÄŸit, diÄŸerlerini dondur
        for layer in base_model.layers[:-20]:  # Son 20 katman hariÃ§ dondur
            layer.trainable = False
    elif strategy == 'full_training':
        base_model.trainable = True   # TÃ¼m katmanlarÄ± eÄŸit

    # Model mimarisi oluÅŸtur
    model = models.Sequential([
        layers.Input(shape=input_shape),

        # Veri augmentation (opsiyonel)
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.1),

        # Preprocessing - Lambda layer ile fonksiyonu sarmalayÄ±n
        layers.Lambda(lambda x: preprocess_func(x)),

        # Base model
        base_model,

        # Custom classifier head
        layers.GlobalAveragePooling2D(),
        layers.Dropout(0.3),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(num_classes, activation='softmax')

    ], name=f"Transfer_{base_model_name}_{strategy}")

    return model

print("ğŸš€ TRANSFER LEARNING KARÅILAÅTIRMASI")
print("=" * 60)

# Test edilecek modeller ve stratejiler
transfer_configs = {
    'mobilenetv2_feature': ('mobilenetv2', 'feature_extraction', 'MobileNetV2 (Feature Extraction)'),
    'mobilenetv2_finetune': ('mobilenetv2', 'fine_tuning', 'MobileNetV2 (Fine-Tuning)'),
    'vgg16_feature': ('vgg16', 'feature_extraction', 'VGG16 (Feature Extraction)'),
    'resnet50_feature': ('resnet50', 'feature_extraction', 'ResNet50 (Feature Extraction)')
}

print("ğŸ¯ Test edilecek transfer learning konfigÃ¼rasyonlarÄ±:")
for config_key, (base_name, strategy, display_name) in transfer_configs.items():
    print(f"   ğŸ“Œ {display_name}")
print()

transfer_results = {}
transfer_times = {}
transfer_params = {}

for i, (config_key, (base_name, strategy, display_name)) in enumerate(transfer_configs.items()):
    print(f"ğŸ”„ Test {i+1}/{len(transfer_configs)}: {display_name}")

    try:
        # Model oluÅŸtur
        model = create_transfer_model(
            base_model_name=base_name,
            input_shape=IMG_SIZE + (3,),
            num_classes=num_classes,
            strategy=strategy
        )

        # Model bilgileri
        total_params = model.count_params()
        trainable_params = sum([tf.reduce_prod(var.shape) for var in model.trainable_variables])
        transfer_params[display_name] = {'total': total_params, 'trainable': trainable_params}

        print(f"   ğŸ“Š Toplam parametre: {total_params:,}")
        print(f"   ğŸ¯ EÄŸitilebilir parametre: {trainable_params:,}")
        print(f"   â„ï¸ DondurulmuÅŸ parametre: {total_params - trainable_params:,}")

        # Optimizer ayarlarÄ± (strategy'ye gÃ¶re)
        if strategy == 'feature_extraction':
            optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
        elif strategy == 'fine_tuning':
            optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)  # Daha dÃ¼ÅŸÃ¼k lr
        else:  # full_training
            optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)

        # Modeli derle
        model.compile(
            optimizer=optimizer,
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

        # EÄŸitim
        start_time = time.time()

        # Epoch sayÄ±sÄ± strategy'ye gÃ¶re ayarla
        epochs = 3 if strategy == 'feature_extraction' else 5

        history = model.fit(
            train_ds,
            epochs=epochs,
            validation_data=val_ds,
            verbose=0
        )

        end_time = time.time()

        # SonuÃ§larÄ± kaydet
        transfer_results[display_name] = history.history
        transfer_times[display_name] = end_time - start_time

        final_val_acc = history.history['val_accuracy'][-1]
        training_time = end_time - start_time

        print(f"   âœ… Final Val Accuracy: %{final_val_acc*100:.2f}")
        print(f"   â° EÄŸitim sÃ¼resi: {training_time:.1f} saniye")
        print(f"   âš¡ Epoch baÅŸÄ±na: {training_time/epochs:.1f}s")
        print()

        # Memory temizliÄŸi
        del model
        K.clear_session()

    except Exception as e:
        print(f"   âŒ Hata: {str(e)[:100]}...")
        print()

print("ğŸ Transfer learning testleri tamamlandÄ±!")

import numpy as np
import matplotlib.pyplot as plt

# === 1. Grafik Fonksiyonu ===
def plot_transfer_learning_comparison(results, times, params):
    """
    Transfer learning sonuÃ§larÄ±nÄ± karÅŸÄ±laÅŸtÄ±ran grafikler
    """
    fig, axes = plt.subplots(3, 2, figsize=(16, 18))

    # 1. Validation Accuracy Comparison
    ax1 = axes[0, 0]
    colors = plt.cm.Set1(np.linspace(0, 1, len(results)))

    for i, (model_name, hist) in enumerate(results.items()):
        epochs = range(1, len(hist['val_accuracy']) + 1)
        ax1.plot(epochs, hist['val_accuracy'],
                label=model_name.replace(' (', '\n('),
                marker='o', linewidth=2, color=colors[i])

    ax1.set_title('ğŸ¯ Validation Accuracy KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Validation Accuracy')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1)

    # 2. Training Efficiency (Accuracy vs Time)
    ax2 = axes[0, 1]
    final_accuracies = [float(hist['val_accuracy'][-1]) for hist in results.values()]
    training_times = [float(v) for v in times.values()]

    for i, (name, acc, time_val) in enumerate(zip(results.keys(), final_accuracies, training_times)):
        ax2.scatter(time_val, acc, s=300, alpha=0.7,
                   color=colors[i], label=name.replace(' (', '\n('))
        ax2.annotate(name.split(' (')[0], (time_val, acc),
                    xytext=(5, 5), textcoords='offset points', fontsize=10)

    ax2.set_title('âš¡ EÄŸitim VerimliliÄŸi (Acc vs Time)', fontweight='bold')
    ax2.set_xlabel('EÄŸitim SÃ¼resi (saniye)')
    ax2.set_ylabel('Final Validation Accuracy')
    ax2.grid(True, alpha=0.3)
    ax2.legend()

    # 3. Parameter Efficiency
    ax3 = axes[1, 0]
    model_names = list(results.keys())
    trainable_params = [int(params[name]['trainable']) for name in model_names]
    total_params = [int(params[name]['total']) for name in model_names]

    x_pos = np.arange(len(model_names))
    width = 0.35

    bars1 = ax3.bar(x_pos - width/2, [p/1e6 for p in total_params],
                    width, label='Toplam Parametre', alpha=0.7, color='lightblue')
    bars2 = ax3.bar(x_pos + width/2, [p/1e6 for p in trainable_params],
                    width, label='EÄŸitilebilir Parametre', alpha=0.7, color='orange')

    ax3.set_title('ğŸ“Š Parametre Analizi (Milyon)', fontweight='bold')
    ax3.set_ylabel('Parametre SayÄ±sÄ± (Milyon)')
    ax3.set_xticks(x_pos)
    ax3.set_xticklabels([name.replace(' (', '\n(') for name in model_names], rotation=45, ha='right')
    ax3.legend()
    ax3.grid(True, alpha=0.3, axis='y')

    for bar1, bar2, total, trainable in zip(bars1, bars2, total_params, trainable_params):
        ax3.text(bar1.get_x() + bar1.get_width()/2., bar1.get_height() + 0.1,
                f'{total/1e6:.1f}M', ha='center', va='bottom', fontsize=9)
        ax3.text(bar2.get_x() + bar2.get_width()/2., bar2.get_height() + 0.1,
                f'{trainable/1e6:.1f}M', ha='center', va='bottom', fontsize=9)

    # 4. Learning Speed (First vs Last Epoch)
    ax4 = axes[1, 1]
    learning_improvements = []
    for model_name, hist in results.items():
        first_epoch_acc = float(hist['val_accuracy'][0])
        last_epoch_acc = float(hist['val_accuracy'][-1])
        improvement = last_epoch_acc - first_epoch_acc
        learning_improvements.append(improvement)

    colors_imp = ['green' if imp > 0.2 else 'orange' if imp > 0.1 else 'red'
                  for imp in learning_improvements]

    bars = ax4.bar(range(len(model_names)), learning_improvements,
                   color=colors_imp, alpha=0.7)
    ax4.set_title('ğŸ“ˆ Ã–ÄŸrenme Ä°yileÅŸtirmesi (Ä°lk vs Son)', fontweight='bold')
    ax4.set_ylabel('Accuracy ArtÄ±ÅŸÄ±')
    ax4.set_xticks(range(len(model_names)))
    ax4.set_xticklabels([name.replace(' (', '\n(') for name in model_names], rotation=45, ha='right')

    for bar, imp in zip(bars, learning_improvements):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                f'{imp:.3f}', ha='center', va='bottom', fontweight='bold')
    ax4.grid(True, alpha=0.3)

    # 5. Model Complexity vs Performance
    ax5 = axes[2, 0]
    complexity_scores = [trainable/1e6 for trainable in trainable_params]

    for i, (name, acc, complexity) in enumerate(zip(model_names, final_accuracies, complexity_scores)):
        ax5.scatter(complexity, acc, s=300, alpha=0.7,
                   color=colors[i], label=name.replace(' (', '\n('))
        ax5.annotate(name.split(' (')[0], (complexity, acc),
                    xytext=(5, 5), textcoords='offset points', fontsize=10)

    ax5.set_title('ğŸ”¬ Model KarmaÅŸÄ±klÄ±ÄŸÄ± vs Performans', fontweight='bold')
    ax5.set_xlabel('EÄŸitilebilir Parametre (Milyon)')
    ax5.set_ylabel('Final Validation Accuracy')
    ax5.grid(True, alpha=0.3)
    ax5.legend()

    # 6. Strategy Comparison
    ax6 = axes[2, 1]
    feature_extraction_results, fine_tuning_results, fe_names, ft_names = [], [], [], []

    for name, hist in results.items():
        final_acc = float(hist['val_accuracy'][-1])
        if 'Feature Extraction' in name:
            feature_extraction_results.append(final_acc)
            fe_names.append(name.split(' (')[0])
        elif 'Fine-Tuning' in name:
            fine_tuning_results.append(final_acc)
            ft_names.append(name.split(' (')[0])

    base_models, fe_scores, ft_scores = [], [], []
    for fe_name, fe_score in zip(fe_names, feature_extraction_results):
        for ft_name, ft_score in zip(ft_names, fine_tuning_results):
            if fe_name == ft_name:
                base_models.append(fe_name)
                fe_scores.append(fe_score)
                ft_scores.append(ft_score)
                break

    if base_models:
        x_pos = np.arange(len(base_models))
        width = 0.35

        bars1 = ax6.bar(x_pos - width/2, fe_scores, width,
                       label='Feature Extraction', alpha=0.7, color='lightcoral')
        bars2 = ax6.bar(x_pos + width/2, ft_scores, width,
                       label='Fine-Tuning', alpha=0.7, color='lightgreen')

        ax6.set_title('ğŸ”„ Strateji KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontweight='bold')
        ax6.set_ylabel('Final Validation Accuracy')
        ax6.set_xticks(x_pos)
        ax6.set_xticklabels(base_models, rotation=45, ha='right')
        ax6.legend()
        ax6.grid(True, alpha=0.3, axis='y')
        ax6.set_ylim(0, 1)

        for bar1, bar2, fe_score, ft_score in zip(bars1, bars2, fe_scores, ft_scores):
            ax6.text(bar1.get_x() + bar1.get_width()/2., bar1.get_height() + 0.01,
                    f'{fe_score:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')
            ax6.text(bar2.get_x() + bar2.get_width()/2., bar2.get_height() + 0.01,
                    f'{ft_score:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')
    else:
        ax6.text(0.5, 0.5, 'Strateji karÅŸÄ±laÅŸtÄ±rmasÄ± iÃ§in\nyeterli data yok',
                ha='center', va='center', transform=ax6.transAxes, fontsize=12)
        ax6.set_title('ğŸ”„ Strateji KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontweight='bold')

    plt.tight_layout()
    plt.suptitle('ğŸš€ Transfer Learning KarÅŸÄ±laÅŸtÄ±rmasÄ±',
                 fontsize=16, fontweight='bold', y=0.98)
    plt.show()

# === 2. Analiz Raporu ===
def analyze_transfer_learning(results, times, params):
    print("ğŸ“Š TRANSFER LEARNING ANALÄ°ZÄ°")
    print("=" * 60)

    best_transfer_model = ""
    best_transfer_performance = 0
    most_efficient_transfer = ""
    best_transfer_efficiency = 0

    for model_name, hist in results.items():
        final_val_acc = float(hist['val_accuracy'][-1])
        training_time = float(times[model_name])
        total_params = int(params[model_name]['total'])
        trainable_params = int(params[model_name]['trainable'])

        param_efficiency = final_val_acc / (trainable_params / 1e6) if trainable_params > 0 else 0
        time_efficiency = final_val_acc / (training_time / 60) if training_time > 0 else 0
        transfer_ratio = trainable_params / total_params if total_params > 0 else 0
        learning_improvement = float(hist['val_accuracy'][-1]) - float(hist['val_accuracy'][0])

        print(f"ğŸš€ {model_name}:")
        print(f"   ğŸ“Š Final Val Accuracy: %{final_val_acc*100:.2f}")
        print(f"   ğŸ§® Toplam parametre: {total_params/1e6:.1f}M")
        print(f"   ğŸ¯ EÄŸitilebilir parametre: {trainable_params/1e6:.1f}M (%{transfer_ratio*100:.1f})")
        print(f"   â° EÄŸitim sÃ¼resi: {training_time:.1f} saniye")
        print(f"   âš¡ Parametre verimliliÄŸi: {param_efficiency:.2f}")
        print(f"   ğŸš€ Zaman verimliliÄŸi: {time_efficiency:.2f}")
        print(f"   ğŸ“ˆ Ã–ÄŸrenme iyileÅŸtirmesi: %{learning_improvement*100:.2f}")
        print()

        if final_val_acc > best_transfer_performance:
            best_transfer_performance = final_val_acc
            best_transfer_model = model_name
        if param_efficiency > best_transfer_efficiency:
            best_transfer_efficiency = param_efficiency
            most_efficient_transfer = model_name

    print("ğŸ† TRANSFER LEARNING SONUÃ‡LARI:")
    print(f"   ğŸ¥‡ En Ä°yi Performans: {best_transfer_model}")
    print(f"     ğŸ“Š Accuracy: %{best_transfer_performance*100:.2f}")
    print(f"   âš¡ En Verimli Model: {most_efficient_transfer}")
    print(f"     ğŸ¯ Verimlilik: {best_transfer_efficiency:.2f}")
    print()

    print("ğŸ’¡ TRANSFER LEARNING REHBERÄ°:")
    print("   ğŸ“± MobileNetV2: Hafif, mobil uygulamalar iÃ§in ideal")
    print("   ğŸ›ï¸ VGG16: Klasik, anlaÅŸÄ±lÄ±r mimari, feature extraction'da iyi")
    print("   ğŸ”„ ResNet50: Derin aÄŸlar, Ã§ok karmaÅŸÄ±k Ã¶zellikler")
    print("   âš¡ EfficientNet: SOTA performans, optimize edilmiÅŸ")
    print()
    print("   ğŸ¯ Feature Extraction: Az veri, hÄ±zlÄ± eÄŸitim")
    print("   ğŸ”§ Fine-Tuning: Daha fazla veri, daha iyi performans")
    print("   ğŸš€ Full Training: Ã‡ok fazla veri, en iyi performans (pahalÄ±)")

# === KullanÄ±m ===
plot_transfer_learning_comparison(transfer_results, transfer_times, transfer_params)
analyze_transfer_learning(transfer_results, transfer_times, transfer_params)

"""# ğŸ“– PART 8: Hyperparameter Tuning (Hiperparametre Optimizasyonu)
âš™ï¸ Hiperparametreler Nedir?
Hiperparametreler, model mimarisini ve eÄŸitim sÃ¼recini kontrol eden parametrelerdir. Model eÄŸitimi sÄ±rasÄ±nda Ã¶ÄŸrenilmez, Ã¶nceden belirlenir.
Modeli eÄŸitmeden Ã¶nce bizim (insanlarÄ±n) belirlediÄŸi, modelin kendisinin Ã¶ÄŸrenmediÄŸi ayarlar. (Ã–rnek: Ã–ÄŸrenme OranÄ±, Batch Boyutu, Katman sayÄ±sÄ±).

ğŸ›ï¸ Ana Hiperparametre Kategorileri:

* **Model Mimarisi**

    Katman sayÄ±sÄ±, nÃ¶ron sayÄ±sÄ±

  
    Aktivasyon fonksiyonlarÄ±

  
    Dropout oranlarÄ±


* **EÄŸitim SÃ¼reci**

    Learning rate, batch size

  
    Epoch sayÄ±sÄ±, optimizer seÃ§imi

  
    Loss fonksiyonu


* **Regularization**

    L1/L2 katsayÄ±larÄ±

  
    Dropout deÄŸerleri

  
    Early stopping patience
"""

def create_tuning_model(config, input_shape, num_classes):
    """
    Hiperparametre konfigÃ¼rasyonuna gÃ¶re model oluÅŸturur
    """
    model = models.Sequential([
        layers.Input(shape=input_shape),
        layers.Rescaling(1./255),
        layers.Flatten(),

        # Ä°lk gizli katman
        layers.Dense(config['hidden_1'], activation=config['activation']),
        layers.Dropout(config['dropout_1']),

        # Ä°kinci gizli katman (opsiyonel)
        layers.Dense(config['hidden_2'], activation=config['activation']),
        layers.Dropout(config['dropout_2']),

        # Ã‡Ä±kÄ±ÅŸ katmanÄ±
        layers.Dense(num_classes, activation='softmax')
    ], name=f"Tuned_Model")

    return model

print("âš™ï¸ HÄ°PERPARAMETRE TUNÄ°NG KARÅILAÅTIRMASI")
print("=" * 60)

# Test edilecek hiperparametre konfigÃ¼rasyonlarÄ±
hyperparameter_configs = {
    'baseline': {
        'hidden_1': 128,
        'hidden_2': 64,
        'activation': 'relu',
        'dropout_1': 0.3,
        'dropout_2': 0.2,
        'learning_rate': 0.001,
        'batch_size': 32,
        'optimizer': 'adam'
    },
    'large_model': {
        'hidden_1': 256,
        'hidden_2': 128,
        'activation': 'relu',
        'dropout_1': 0.4,
        'dropout_2': 0.3,
        'learning_rate': 0.001,
        'batch_size': 32,
        'optimizer': 'adam'
    },
    'high_dropout': {
        'hidden_1': 128,
        'hidden_2': 64,
        'activation': 'relu',
        'dropout_1': 0.6,
        'dropout_2': 0.5,
        'learning_rate': 0.001,
        'batch_size': 32,
        'optimizer': 'adam'
    },
    'small_lr': {
        'hidden_1': 128,
        'hidden_2': 64,
        'activation': 'relu',
        'dropout_1': 0.3,
        'dropout_2': 0.2,
        'learning_rate': 0.0001,
        'batch_size': 32,
        'optimizer': 'adam'
    },
    'large_batch': {
        'hidden_1': 128,
        'hidden_2': 64,
        'activation': 'relu',
        'dropout_1': 0.3,
        'dropout_2': 0.2,
        'learning_rate': 0.002,  # BÃ¼yÃ¼k batch iÃ§in lr artÄ±rÄ±ldÄ±
        'batch_size': 64,
        'optimizer': 'adam'
    },
    'tanh_activation': {
        'hidden_1': 128,
        'hidden_2': 64,
        'activation': 'tanh',
        'dropout_1': 0.3,
        'dropout_2': 0.2,
        'learning_rate': 0.001,
        'batch_size': 32,
        'optimizer': 'adam'
    }
}

print("ğŸ¯ Test edilecek konfigÃ¼rasyonlar:")
for config_name, config in hyperparameter_configs.items():
    print(f"   ğŸ“Œ {config_name}: Hidden({config['hidden_1']}, {config['hidden_2']}), "
          f"LR({config['learning_rate']}), Batch({config['batch_size']})")
print()

tuning_results = {}
tuning_times = {}
tuning_configs = {}

for i, (config_name, config) in enumerate(hyperparameter_configs.items()):
    print(f"ğŸ”„ Test {i+1}/{len(hyperparameter_configs)}: {config_name}")

    # Model oluÅŸtur
    model = create_tuning_model(
        config=config,
        input_shape=IMG_SIZE + (3,),
        num_classes=num_classes
    )

    # Optimizer oluÅŸtur
    if config['optimizer'] == 'adam':
        optimizer = tf.keras.optimizers.Adam(learning_rate=config['learning_rate'])
    elif config['optimizer'] == 'sgd':
        optimizer = tf.keras.optimizers.SGD(learning_rate=config['learning_rate'])
    elif config['optimizer'] == 'rmsprop':
        optimizer = tf.keras.optimizers.RMSprop(learning_rate=config['learning_rate'])

    # Modeli derle
    model.compile(
        optimizer=optimizer,
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    # Batch size'a gÃ¶re dataset'i yeniden oluÅŸtur (gerekirse)
    if config['batch_size'] != BATCH_SIZE:
        temp_train_ds = tf.keras.preprocessing.image_dataset_from_directory(
            processed_data_dir,
            validation_split=VALIDATION_SPLIT,
            subset="training",
            seed=RANDOM_SEED,
            image_size=IMG_SIZE,
            batch_size=config['batch_size'],
            label_mode='int'
        ).cache().prefetch(AUTOTUNE)

        temp_val_ds = tf.keras.preprocessing.image_dataset_from_directory(
            processed_data_dir,
            validation_split=VALIDATION_SPLIT,
            subset="validation",
            seed=RANDOM_SEED,
            image_size=IMG_SIZE,
            batch_size=config['batch_size'],
            label_mode='int'
        ).cache().prefetch(AUTOTUNE)
    else:
        temp_train_ds = train_ds
        temp_val_ds = val_ds

    # EÄŸitim
    start_time = time.time()
    history = model.fit(
        temp_train_ds,
        epochs=4,  # HÄ±zlÄ± karÅŸÄ±laÅŸtÄ±rma iÃ§in
        validation_data=temp_val_ds,
        verbose=0
    )
    end_time = time.time()

    # SonuÃ§larÄ± kaydet
    tuning_results[config_name] = history.history
    tuning_times[config_name] = end_time - start_time
    tuning_configs[config_name] = config

    final_val_acc = history.history['val_accuracy'][-1]
    training_time = end_time - start_time

    print(f"   âœ… Final Val Accuracy: %{final_val_acc*100:.2f}")
    print(f"   â° EÄŸitim sÃ¼resi: {training_time:.1f} saniye")
    print(f"   ğŸ“Š Parametre sayÄ±sÄ±: {model.count_params():,}")
    print()

    # Memory temizliÄŸi
    del model
    if config['batch_size'] != BATCH_SIZE:
        del temp_train_ds, temp_val_ds
    K.clear_session()

print("ğŸ Hiperparametre tuning testleri tamamlandÄ±!")

import matplotlib.pyplot as plt
import numpy as np

def plot_tuning_comparison(results, times, configs):
    """
    Hiperparametre tuning sonuÃ§larÄ±nÄ± karÅŸÄ±laÅŸtÄ±ran grafikler.
    """
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))

    model_names = list(results.keys())
    colors = plt.cm.tab10(np.linspace(0, 1, len(model_names)))

    # 1. Validation Accuracy Comparison
    ax1 = axes[0, 0]
    for i, model_name in enumerate(model_names):
        epochs = range(1, len(results[model_name]['val_accuracy']) + 1)
        ax1.plot(epochs, results[model_name]['val_accuracy'],
                 label=model_name, marker='o', linewidth=2, color=colors[i])

    ax1.set_title('ğŸ¯ Validation Accuracy KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Validation Accuracy')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1)

    # 2. Training Efficiency (Accuracy vs Time)
    ax2 = axes[0, 1]
    final_accuracies = [float(results[name]['val_accuracy'][-1]) for name in model_names]
    training_times = [float(times[name]) for name in model_names]

    for i, (name, acc, time_val) in enumerate(zip(model_names, final_accuracies, training_times)):
        ax2.scatter(time_val, acc, s=300, alpha=0.7, color=colors[i], label=name)
        ax2.annotate(name, (time_val, acc), xytext=(5, 5), textcoords='offset points', fontsize=9)

    ax2.set_title('âš¡ EÄŸitim VerimliliÄŸi (Acc vs Time)', fontweight='bold')
    ax2.set_xlabel('EÄŸitim SÃ¼resi (saniye)')
    ax2.set_ylabel('Final Validation Accuracy')
    ax2.grid(True, alpha=0.3)
    ax2.legend()

    # 3. Parametre SayÄ±sÄ± KarÅŸÄ±laÅŸtÄ±rmasÄ±
    ax3 = axes[1, 0]
    param_counts = [configs[name]['hidden_1']*configs[name]['hidden_2'] + configs[name]['hidden_2']*len(results[name]['val_accuracy']) for name in model_names]
    bars = ax3.bar(model_names, param_counts, color=colors, alpha=0.7)

    ax3.set_title('ğŸ“Š Parametre SayÄ±sÄ± KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontweight='bold')
    ax3.set_ylabel('Parametre SayÄ±sÄ±')
    ax3.grid(True, axis='y', alpha=0.3)

    for bar, param_count in zip(bars, param_counts):
        ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05*max(param_counts),
                 f"{param_count:,}", ha='center', va='bottom', fontsize=9)

    # 4. Learning Improvement (Final - Ä°lk Epoch)
    ax4 = axes[1, 1]
    learning_improvements = [
        float(results[name]['val_accuracy'][-1]) - float(results[name]['val_accuracy'][0])
        for name in model_names
    ]
    bars = ax4.bar(model_names, learning_improvements, color=colors, alpha=0.7)

    ax4.set_title('ğŸ“ˆ Ã–ÄŸrenme Ä°yileÅŸtirmesi (Final - Ä°lk Epoch)', fontweight='bold')
    ax4.set_ylabel('Accuracy ArtÄ±ÅŸÄ±')
    ax4.grid(True, axis='y', alpha=0.3)

    for bar, improvement in zip(bars, learning_improvements):
        ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.005,
                 f"{improvement:.3f}", ha='center', va='bottom', fontsize=9)

    plt.tight_layout()
    plt.suptitle('ğŸš€ Hiperparametre Tuning KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontsize=16, fontweight='bold', y=1.02)
    plt.show()
def analyze_tuning_results(results, times, configs):
    print("ğŸ“Š HÄ°PERPARAMETRE TUNING ANALÄ°ZÄ°")
    print("=" * 60)

    best_model = ""
    best_acc = 0
    fastest_model = ""
    best_efficiency = 0

    for name in results.keys():
        final_val_acc = float(results[name]['val_accuracy'][-1])
        first_val_acc = float(results[name]['val_accuracy'][0])
        training_time = float(times[name])
        improvement = final_val_acc - first_val_acc
        efficiency = final_val_acc / training_time if training_time > 0 else 0

        print(f"ğŸ”¹ {name}:")
        print(f"   ğŸ“Š Final Val Accuracy: %{final_val_acc*100:.2f}")
        print(f"   â° EÄŸitim sÃ¼resi: {training_time:.1f} saniye")
        print(f"   ğŸ“ˆ Ã–ÄŸrenme iyileÅŸtirmesi: %{improvement*100:.2f}")
        print(f"   âš¡ Verimlilik (Acc/SÃ¼re): {efficiency:.4f}")
        print()

        if final_val_acc > best_acc:
            best_acc = final_val_acc
            best_model = name
        if efficiency > best_efficiency:
            best_efficiency = efficiency
            fastest_model = name

    print("ğŸ† SONUÃ‡LAR:")
    print(f"   ğŸ¥‡ En iyi model: {best_model} (%{best_acc*100:.2f})")
    print(f"   âš¡ En verimli model: {fastest_model} (Efficiency: {best_efficiency:.4f})")
plot_tuning_comparison(tuning_results, tuning_times, tuning_configs)
analyze_tuning_results(tuning_results, tuning_times, tuning_configs)





